{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E35Va__CQ04h"
      },
      "source": [
        "## Notebook Summary\n",
        "* Learn a Language Model on the ROC Story Dataset: https://cs.rochester.edu/nlp/rocstories/\n",
        "> Available here: https://drive.google.com/file/d/1eJINcSbC3JLl0hTNbhh5G94zTuXinpC-/view?usp=sharing\n",
        "\n",
        "* Generate Text with this Language Model using several decoding techniques\n",
        "* Evaluate the Language Model using the perplexity and the BLEU score. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v5u6xAJRm40"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Qmjnv8RRlt"
      },
      "source": [
        "### 1. Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTiJALLECm0u"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/12_Teaching/UM6P-NLP-Jan2022/notebooks/ROCStories_winter2017.csv\"\n",
        "df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSA-vgXVfjnG",
        "outputId": "5750809e-06a6-4307-e3d2-ddccfe95c8bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                storyid  ...                                          sentence5\n",
            "0  8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd  ...  After a few weeks, he started to feel much bet...\n",
            "1  0beabab2-fb49-460e-a6e6-f35a202e3348  ...  Tom sat on his couch filled with regret about ...\n",
            "2  87da1a22-df0b-410c-b186-439700b70ba6  ...  Marcus was happy to have the right clothes for...\n",
            "3  2d16bcd6-692a-4fc0-8e7c-4a6f81d9efa9  ...  He ended up buying the truck he wanted despite...\n",
            "4  c71bb23b-7731-4233-8298-76ba6886cee1  ...      His congregation was delighted and so was he.\n",
            "\n",
            "[5 rows x 7 columns]\n",
            "52665\n"
          ]
        }
      ],
      "source": [
        "print(df.head())\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0BluoGfDv-p"
      },
      "outputs": [],
      "source": [
        "def get_sentences(df, max_samples=None):\n",
        "    df[\"sentence_1_2\"] = df.sentence1 + \" \" + df.sentence2\n",
        "    sentences = df.sentence_1_2\n",
        "    sentences_1, sentences_2 = df[\"sentence1\"], df[\"sentence2\"]\n",
        "    if max_samples is not None:\n",
        "        sentences = sentences[:max_samples]\n",
        "        sentences_1 = sentences_1[:max_samples]\n",
        "        sentences_2 = sentences_2[:max_samples]\n",
        "    return sentences, sentences_1, sentences_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWal4b8hRW-g"
      },
      "outputs": [],
      "source": [
        "sentences, sentences_1, sentences_2 = get_sentences(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pqzrPZ_9SNLR",
        "outputId": "6403c90f-8634-4d2b-c9d4-867a4a1c1c7c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I was on duty at my work when i noticed someone staring at me. I didn't mind him but the day after that i saw him again.\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print sentences example:\n",
        "sentences[np.random.randint(len(sentences))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36FAXVWNR37P"
      },
      "source": [
        "**Exercise 1**: Create a function `clean_text` that clean sentences\n",
        "* split words with \"-\"\n",
        "* split number and text using a regular expressions and the function `re.split`\n",
        "* Replace the token \"&\" by the token \"and\". \n",
        "* lower all letters\n",
        "* Tips: create lambda functions and apply it to the dataframe using `.apply` method. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-UbiKEbD_iy"
      },
      "outputs": [],
      "source": [
        "def clean_text(sentences):\n",
        "    clean_func1 = lambda t: ' '.join(t.split(\"-\")) # .replace(\"-\", \" \")\n",
        "    clean_func2 = lambda t: ' '.join(re.split(r\"([0-9]+)([a-z]+)\", t, flags=re.I)) # \"9st\" => \"9 st\" \n",
        "    clean_func3 = lambda t: ' '.join(re.split(r\"([a-z]+)([0-9]+)\", t, flags=re.I))\n",
        "    clean_func4 = lambda t: t.lower().replace(\"&\", \"and\")\n",
        "    sentences = sentences.apply(clean_func1)\n",
        "    sentences = sentences.apply(clean_func2)\n",
        "    sentences = sentences.apply(clean_func3)\n",
        "    sentences = sentences.apply(clean_func4)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdaxDDRyT3Lg"
      },
      "outputs": [],
      "source": [
        "sentences = clean_text(sentences)\n",
        "sentences_1 = clean_text(sentences_1)\n",
        "sentences_2 = clean_text(sentences_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GsDk0SkS-wG"
      },
      "source": [
        "**Exercise 2**: Build the vocab by removing some punctuation and adding the special tokens. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXPa_EsRULDU",
        "outputId": "ec7b6462-eef7-488c-d828-b66177a8906f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AWJMCv2EBiA"
      },
      "outputs": [],
      "source": [
        "def get_vocab(sentences, tokens_to_remove=[\"$\", \"%\", \"'\", \"''\"], special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\"]):\n",
        "    print(\"Building vocab....\")\n",
        "    # tokenize sentences\n",
        "    tokenized_sentences = sentences.apply(word_tokenize)\n",
        "    tokenized_sentences = tokenized_sentences.values # nested list \n",
        "    tokens = [w for s in tokenized_sentences for w in s] # flatten list \n",
        "\n",
        "    # build vocab\n",
        "    unique_tokens = list(set(tokens))\n",
        "    for token in tokens_to_remove:\n",
        "        unique_tokens.remove(token)\n",
        "    unique_tokens.sort()\n",
        "    vocab = {v: k for k, v in enumerate(special_tokens + unique_tokens)}\n",
        "    print(\"vocab length:\", len(vocab))\n",
        "    print(\"saving vocab...\")\n",
        "    with open(\"vocab.json\", \"w\") as f:\n",
        "        json.dump(vocab, f)\n",
        "    return tokens, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYiFrF5DUTXM",
        "outputId": "e86b3859-8a06-4336-c16e-bc1214aa5724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building vocab....\n",
            "vocab length: 20093\n",
            "saving vocab...\n"
          ]
        }
      ],
      "source": [
        "tokens, vocab = get_vocab(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDesaF9CEYZT"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentences, vocab):\n",
        "    # tokenize sentences with vocab\n",
        "    # add <SOS> and <EOS> at beginning and end of sentence\n",
        "    # splitter input data, target data (shifted sentence)\n",
        "    # pad sequences to have same length \n",
        "    tok_func = lambda t: [vocab[\"<SOS>\"]] + [vocab[w] for w in t if w in vocab.keys()]+[vocab[\"<EOS>\"]]\n",
        "    tokens_id = sentences.apply(word_tokenize)\n",
        "    tokens_id = tokens_id.apply(tok_func)\n",
        "    df = pd.DataFrame()\n",
        "    df['input_sentence'] = tokens_id.apply(lambda t: t[:-1])\n",
        "    df['target_sentence'] = tokens_id.apply(lambda t: t[1:])\n",
        "    len_sentences = tokens_id.apply(len)\n",
        "    max_len = np.max(len_sentences)\n",
        "    pad_func = lambda t: t + [0] * (max_len - len(t))\n",
        "    df[\"input_sentence\"] = df.input_sentence.apply(pad_func)\n",
        "    df[\"target_sentence\"] = df.target_sentence.apply(pad_func)\n",
        "    return df, max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzMP2JlgUysi"
      },
      "outputs": [],
      "source": [
        "df, max_len = tokenize(sentences, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO92-7vRxS9B",
        "outputId": "1ca5b30f-6e9f-49bb-c6e4-cf8f99b96bde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_sentence     [1, 4725, 12150, 8137, 7930, 13993, 12354, 326...\n",
            "target_sentence    [4725, 12150, 8137, 7930, 13993, 12354, 326, 1...\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pprint.pprint(df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LE1cfJmcEc3K"
      },
      "outputs": [],
      "source": [
        "def tokenize_test(sentences, vocab):\n",
        "    tokenize_func = lambda t: word_tokenize(t)\n",
        "    tok_to_id_func = lambda t: [vocab[\"<SOS>\"]]+[vocab[w] for w in t if w in vocab.keys()]+[vocab[\"<EOS>\"]]\n",
        "    tokenized_sentences = sentences.apply(tokenize_func)\n",
        "    tokens_id = tokenized_sentences.apply(tok_to_id_func)\n",
        "    len_sentences = tokens_id.apply(len)\n",
        "    return tokens_id, len_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg1VZ2ZdFFsA"
      },
      "outputs": [],
      "source": [
        "def split_train_test(sentences, sentences_1_and_2, val_size=5000, test_size=3000):\n",
        "    train_size = len(sentences) - (val_size + test_size)\n",
        "    train_sentences = sentences[:train_size]\n",
        "    val_sentences = sentences[train_size:train_size + val_size]\n",
        "    test_sentences = sentences_1_and_2[train_size + val_size:train_size + val_size + test_size]\n",
        "    return train_sentences, val_sentences, test_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PatJzbzFQcE"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data_path):\n",
        "    df = pd.read_csv(data_path) # read the file\n",
        "    sentences, sentences_1, sentences_2 = get_sentences(df) # select the first 2 sentences\n",
        "    sentences, sentences_1, sentences_2 = clean_text(sentences), clean_text(sentences_1), clean_text(sentences_2) # text cleaning\n",
        "    tokens, vocab = get_vocab(sentences) # Build vocab \n",
        "    padded_sentences, max_len = tokenize(sentences, vocab) # tokenize, split input/target, pad sequences\n",
        "    print(\"dataset set length:\", len(padded_sentences))\n",
        "    sentences_1, len_sentences_1 = tokenize_test(sentences_1, vocab)\n",
        "    sentences_2, len_sentences_2 = tokenize_test(sentences_2, vocab)\n",
        "    sentences_1_and_2 = pd.concat([sentences_1, sentences_2], axis=1) # dataframe with 2 sentences \n",
        "    train_sentences, val_sentences, test_sentences = split_train_test(padded_sentences, sentences_1_and_2)\n",
        "    print(\"train dataset size\", len(train_sentences))\n",
        "    print(\"val dataset size\", len(val_sentences))\n",
        "    print(\"test dataset size\", len(test_sentences))\n",
        "    return train_sentences, val_sentences, test_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AEoxxl-2p2R",
        "outputId": "238291f4-9f2a-4082-bc5e-046bf9337b19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building vocab....\n",
            "vocab length: 20094\n",
            "saving vocab...\n",
            "dataset set length: 52665\n",
            "train dataset size 44665\n",
            "val dataset size 5000\n",
            "test dataset size 3000\n"
          ]
        }
      ],
      "source": [
        "data_path = \"/content/drive/MyDrive/12_Teaching/UM6P-NLP-Jan2022/notebooks/ROCStories_winter2017.csv\"\n",
        "train_sentences, val_sentences, test_sentences = preprocess_data(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLhNTRFUF2xI"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(dataset, max_samples, batch_size):\n",
        "    # transform 2 columns of dataframe to numpy arrays\n",
        "    input_sentence = np.array([seq for seq in dataset.input_sentence.values])\n",
        "    target_sentence = np.array([seq for seq in dataset.target_sentence.values])\n",
        "    if max_samples is not None:\n",
        "      input_sentence = input_sentence[:max_samples]\n",
        "      target_sentence = target_sentence[:max_samples]\n",
        "    # tensorflow dataset\n",
        "    tfdataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (input_sentence, target_sentence))\n",
        "    # tensorflow dataloader\n",
        "    dataloader = tfdataset.batch(batch_size, drop_remainder=True)\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4nIvqkUGwCx"
      },
      "outputs": [],
      "source": [
        "def get_test_dataloader(data):\n",
        "    inputs, targets = data.sentence1, data.sentence2\n",
        "    inputs = inputs.to_list()\n",
        "    targets = targets.to_list()\n",
        "    inputs = [tf.constant(inp, dtype=tf.int32) for inp in inputs]\n",
        "    targets = [tf.constant(tar, dtype=tf.int32) for tar in targets]\n",
        "    return (inputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yv4-qi43ZIi",
        "outputId": "9cbf2372-8e80-41a5-e478-480a837764fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
            "array([[    1,  4726, 12151, ...,     0,     0,     0],\n",
            "       [    1, 18245,  7931, ...,     0,     0,     0],\n",
            "       [    1, 10910, 11919, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [    1,  9261,  7931, ...,     0,     0,     0],\n",
            "       [    1,  9503, 19398, ...,     0,     0,     0],\n",
            "       [    1,  9683, 12758, ...,     0,     0,     0]])>, <tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
            "array([[ 4726, 12151,  8138, ...,     0,     0,     0],\n",
            "       [18245,  7931,   327, ...,     0,     0,     0],\n",
            "       [10910, 11919,  3640, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [ 9261,  7931,   829, ...,     0,     0,     0],\n",
            "       [ 9503, 19398, 18206, ...,     0,     0,     0],\n",
            "       [ 9683, 12758,  8257, ...,     0,     0,     0]])>)\n",
            "(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
            "array([[    1,  9261,   890, ...,     0,     0,     0],\n",
            "       [    1, 11337,   829, ...,     0,     0,     0],\n",
            "       [    1, 19693, 19436, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [    1,  5095, 10610, ...,     0,     0,     0],\n",
            "       [    1,   896, 11919, ...,     0,     0,     0],\n",
            "       [    1,  9644,  7753, ...,     0,     0,     0]])>, <tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n",
            "array([[ 9261,   890,  8356, ...,     0,     0,     0],\n",
            "       [11337,   829,  7617, ...,     0,     0,     0],\n",
            "       [19693, 19436, 12355, ...,     0,     0,     0],\n",
            "       ...,\n",
            "       [ 5095, 10610,  6784, ...,     0,     0,     0],\n",
            "       [  896, 11919, 13182, ...,     0,     0,     0],\n",
            "       [ 9644,  7753, 18975, ...,     0,     0,     0]])>)\n",
            "tf.Tensor(\n",
            "[    1  1958  9409 19436 17968  6505  6945   327 13151  5751  4085  1289\n",
            " 17968  6417    21     2], shape=(16,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[    1  1979  1827 19166 15959   890 16189  4256 11807  5746  1208 11713\n",
            "  1208  9409    21     2], shape=(16,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "train_loader = get_dataloader(train_sentences, batch_size=64, max_samples=None)\n",
        "print(next(iter(train_loader)))\n",
        "val_loader = get_dataloader(val_sentences, batch_size=64, max_samples=None)\n",
        "print(next(iter(val_loader)))\n",
        "test_loader = get_test_dataloader(test_sentences)\n",
        "inputs, targets = test_loader\n",
        "print(inputs[0])\n",
        "print(targets[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR8eRggwWjEf"
      },
      "source": [
        "***Exercise 4***: \n",
        "Create a `decode` function that decode a list of tokens id into text using the vocab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjbSjR7FOgU5"
      },
      "outputs": [],
      "source": [
        "def decode(seq_idx, vocab, delim=' ', ignored=[\"<SOS>\", \"<PAD>\", \"<EOS>\"]):\n",
        "  # inv vocab\n",
        "  inv_vocab = {token_id: token for token, token_id in vocab.items()}\n",
        "  # decode sent\n",
        "  decoded_sentence = [inv_vocab[token_id] \n",
        "                      for token_id in seq_idx\n",
        "                      if inv_vocab[token_id] not in ignored]\n",
        "  # join tokens\n",
        "  return delim.join(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "fkm3gfqi8SVe",
        "outputId": "0adc75bb-6b3c-463b-a813-ab7510524ea5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"davidson notices head haddie putrid ona a'neial lotion off weightlifting reception . head examining hispanic habitual toad trying anderson figured outage theater reasonable .\""
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs, targets = (next(iter(train_loader)))\n",
        "decode(inputs[0].numpy(), vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_it0rCW9Wm"
      },
      "source": [
        "**Exercise 5**: Build a GRU network using `tf.keras.Model` or `tf.keras.Sequential` with: \n",
        "* An embedding layer\n",
        "* A GRU layer: there is a subtility -> you need to ouput the whole sequence of hidden states using the return_sequences argument: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "* A dropout layer after the LSTM Layer\n",
        "* A dense layer that project the hidden state over the vocabulary. \n",
        "> What is the size of the NN output ? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW00GI4fL2XA"
      },
      "outputs": [],
      "source": [
        "# Build Model\n",
        "def build_LSTM(vocab_size, emb_size, output_size, rnn_units, dropout_rate, rnn_drop_rate=0.0):\n",
        "  model = tf.keras.Sequential()\n",
        "  e = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=emb_size, mask_zero=True)\n",
        "  model.add(e)\n",
        "  lstm = tf.keras.layers.LSTM(rnn_units, recurrent_dropout=rnn_drop_rate, return_sequences=True) # to output each hidden representation of the sequence\n",
        "  model.add(lstm)\n",
        "  model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "  model.add(tf.keras.layers.Dense(output_size)) # we compute only logits. \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYWGz2YBBlcl"
      },
      "outputs": [],
      "source": [
        "lstm_model = build_LSTM(vocab_size=len(vocab), emb_size=32, output_size=len(vocab), rnn_units=64, dropout_rate=0.1)\n",
        "lr = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(lr,\n",
        "                                                  beta_1=0.9,\n",
        "                                                  beta_2=0.98,\n",
        "                                                  epsilon=1e-9)\n",
        "EPOCHS = 10 # 10 for debugging. generally, in Language modelling, we take between 30 and 50 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4azZDaEWBs8V"
      },
      "outputs": [],
      "source": [
        "# test past forward on the lstm\n",
        "for (inputs, targets) in train_loader.take(1):\n",
        "    preds = lstm_model(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFk-QxiSB4jg",
        "outputId": "0bb6c2bc-f197-4163-a03c-adb948b637d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 38, 20093)\n"
          ]
        }
      ],
      "source": [
        "print(preds.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2hnzIxqXryK"
      },
      "source": [
        "**Exercise 6:**  \n",
        "* Create a function that train LSTM (similarly of notebook of day 2) \n",
        "> Use the tf.keras.losses.SparseCategoricalCrossEntropy: https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
        " \n",
        "\n",
        "* Compute the perplexity over the train and validation set: Note that the perplexity is the exponantial of the cross-entropy ! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIANQGTuGsvw"
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLcZGZufMK3k"
      },
      "outputs": [],
      "source": [
        "def train_LSTM(model, optimizer, EPOCHS, train_dataset, val_dataset, checkpoint_path):\n",
        "    LSTM_ckpt_path = checkpoint_path + '/' + 'LSTM-{epoch}'\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=LSTM_ckpt_path,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=True,\n",
        "            verbose=1)\n",
        "    ]\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    # Save the weights using the `checkpoint_path` format\n",
        "    #model.save_weights(checkpoint_path.format(epoch=0))\n",
        "\n",
        "    # --- starting the training ... -----------------------------------------------\n",
        "    start_training = time.time()\n",
        "    rnn_history = model.fit(train_dataset,\n",
        "                            epochs=EPOCHS,\n",
        "                            validation_data=val_dataset,\n",
        "                            callbacks=callbacks,\n",
        "                            verbose=2)\n",
        "\n",
        "    train_loss_history_rnn = rnn_history.history['loss']\n",
        "    val_loss_history_rnn = rnn_history.history['val_loss']\n",
        "    train_ppl_history = np.exp(train_loss_history_rnn)\n",
        "    val_ppl_history = np.exp(val_loss_history_rnn)\n",
        "    train_history = [train_loss_history_rnn, val_loss_history_rnn, train_ppl_history, val_ppl_history]\n",
        "\n",
        "    print('Training time for {} epochs: {}'.format(EPOCHS, time.time() - start_training))\n",
        "\n",
        "    return train_history # [list_train_loss, list_val_loss, list_train_perplexity, list_val_perplexity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "hm3BG_GvG_G1",
        "outputId": "0ba4d9e2-6fa8-46fe-c544-f1d0d328f750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          642976    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 64)          24832     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 64)          0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 20093)       1306045   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,973,853\n",
            "Trainable params: 1,973,853\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-9dff64f78178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_LSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-55-741dc9d9c337>\u001b[0m in \u001b[0;36mtrain_LSTM\u001b[0;34m(model, optimizer, EPOCHS, train_dataset, val_dataset, checkpoint_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                             verbose=2)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtrain_loss_history_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m:  indices[48,1] = 20093 is not in [0, 20093)\n\t [[node sequential/embedding/embedding_lookup\n (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py:191)\n]] [Op:__inference_train_function_19011]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential/embedding/embedding_lookup:\nIn[0] sequential/embedding/embedding_lookup/17141:\t\nIn[1] sequential/embedding/Cast (defined at /usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py:190)\n\nOperation defined at: (most recent call last)\n>>>   File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n>>>     \"__main__\", mod_spec)\n>>> \n>>>   File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n>>>     handler_func(fileobj, events)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n>>>     self._handle_recv()\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n>>>     self._run_callback(callback, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n>>>     callback(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n>>>     return self.dispatch_shell(stream, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n>>>     handler(stream, idents, msg)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n>>>     user_expressions, allow_stdin)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n>>>     interactivity=interactivity, compiler=compiler, result=result)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n>>>     if self.run_code(code, result):\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"<ipython-input-56-9dff64f78178>\", line 6, in <module>\n>>>     train_history = train_LSTM(model=lstm_model, optimizer=optimizer, EPOCHS=EPOCHS, train_dataset=train_loader, val_dataset=val_loader, checkpoint_path=checkpoint_path)\n>>> \n>>>   File \"<ipython-input-55-741dc9d9c337>\", line 26, in train_LSTM\n>>>     verbose=2)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\", line 373, in call\n>>>     return super(Sequential, self).call(inputs, training=training, mask=mask)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n>>>     inputs, training=training, mask=mask)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py\", line 191, in call\n>>>     out = tf.nn.embedding_lookup(self.embeddings, inputs)\n>>> "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "checkpoint_path = \"/checkpoints\"\n",
        "if not os.path.isdir(checkpoint_path):\n",
        "  os.makedirs(checkpoint_path)\n",
        "train_history = train_LSTM(model=lstm_model, optimizer=optimizer, EPOCHS=EPOCHS, train_dataset=train_loader, val_dataset=val_loader, checkpoint_path=checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r91Tz69YDcl"
      },
      "source": [
        "**Exercise 7**:\n",
        "Create a function that generate text at inference over the trained lstm. \n",
        "This function either use: \n",
        "* greedy decoding using `tf.math.argmax`\n",
        "* sampling with temperature decoding `tf.random.categorical`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ER0_eQqMhog"
      },
      "outputs": [],
      "source": [
        "def generate_text(lstm, inputs, seq_len=10,\n",
        "                            decoding=\"sampling\", temp=1):\n",
        "  # Loop over number of decoding timesteps: (equal to seq_len)\n",
        "\n",
        "      # pass forward on the lstm on inputs\n",
        "\n",
        "      # get the last prediction (logits)\n",
        "\n",
        "      # if decoding = sampling \n",
        "        # divide logits by temperature \n",
        "        # sample a word\n",
        "\n",
        "        # if decoding == \"greedy\"\n",
        "        # find the greedy word (argmax)\n",
        "\n",
        "      # compute the inputs of the next timestep by concatenating inputs and the predicted token using tf.concat\n",
        "\n",
        "  # return the final inputs (complete sequence of word ids)\n",
        "  return inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_sG6xjCZWFg"
      },
      "source": [
        "**Exercise 8**:\n",
        "* Take an `inputs` of the test dataset, generate text on this inputs, and decode it with the `decode` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEADJ9QWZpvZ"
      },
      "source": [
        "#### Measuring the BLEU score \n",
        "(between true sentence and generated sentence on the test dataset)  \n",
        "use sentence_bleu of nltk: https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8yT-n0oQl5M"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJA3l8IIQkS2"
      },
      "outputs": [],
      "source": [
        "def BLEU_score(true_sentence, generated_sentence, split_str=False):\n",
        "    if split_str:\n",
        "        true_sentence = true_sentence.split(sep=' ')\n",
        "        generated_sentence = [generated_sentence.split(sep=' ')]\n",
        "    score = sentence_bleu(references=generated_sentence, hypothesis=true_sentence, smoothing_function=SmoothingFunction().method2)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkJkI4-_Z0gN"
      },
      "source": [
        "**Exercise 9**: Create a function that: \n",
        "* Loop over the test set \n",
        "* generate text on each inputs of the test set\n",
        "* decode it using the decode function \n",
        "* Evaluate the BLEU score between the true decoded sentence (from the test set) and the decoded generate sentence \n",
        "* Compute the average BLEU score on the test set. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "03E_LanguageModelling_ROCDataset_NLP-class_Emines-Jan22.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
