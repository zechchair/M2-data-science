{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8U-NZM-rjsc"
   },
   "source": [
    "# Importing The Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "eyZq7RpPrjsh",
    "outputId": "943dffe7-85d3-4ee2-8ff8-bcc6ef036810"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from IPython.display import clear_output\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode (connected = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ5WMbCqrjsj"
   },
   "source": [
    "# Importing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMJnXxVUrjsk"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('../input/imdb-movie-reviews-dataset/movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ke8CF_oqrjsk",
    "outputId": "5615f388-728c-4b87-8519-592b1da55380"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRHObXa2rjsl"
   },
   "source": [
    "# Making two copies of Reviews to edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMHwFJBvrjsl"
   },
   "outputs": [],
   "source": [
    "# Replacing Positive -> 1 and Negative -> 0\n",
    "\n",
    "data.replace({\"positive\":1,\"negative\":0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMNS4ARJrjsl"
   },
   "outputs": [],
   "source": [
    "#Edits After Removing Stopwords\n",
    "Edited_Review = data['review'].copy()\n",
    "data['Review_without_stopwords'] = Edited_Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btUW9d2yrjsm"
   },
   "source": [
    "# Having a look at 1st five reviews in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "tDr25Khtrjsn",
    "outputId": "cc168e5d-f123-4a85-9eaa-d934484a15be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Review_without_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1   \n",
       "1  OK... so... I really like Kris Kristofferson a...          0   \n",
       "2  ***SPOILER*** Do not read this, if you think a...          0   \n",
       "3  hi for all the people who have seen this wonde...          1   \n",
       "4  I recently bought the DVD, forgetting just how...          0   \n",
       "\n",
       "                            Review_without_stopwords  \n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...  \n",
       "1  OK... so... I really like Kris Kristofferson a...  \n",
       "2  ***SPOILER*** Do not read this, if you think a...  \n",
       "3  hi for all the people who have seen this wonde...  \n",
       "4  I recently bought the DVD, forgetting just how...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlOba2A_rjsn"
   },
   "source": [
    "# Preprocessing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1rrEu5Brjsn"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess Reviews data\n",
    "def preprocess_Reviews_data(data,name):\n",
    "    # Proprocessing the data\n",
    "    data[name]=data[name].str.lower()\n",
    "    # Code to remove the Hashtags from the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
    "    # Code to remove the links from the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "    # Code to remove the Special characters from the text \n",
    "    data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "    # Code to substitute the multiple spaces with single spaces\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "    # Code to remove all the single characters in the text\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "    # Remove the twitter handlers\n",
    "    data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "\n",
    "# Function to tokenize and remove the stopwords    \n",
    "def rem_stopwords_tokenize(data,name):\n",
    "      \n",
    "    def getting(sen):\n",
    "        example_sent = sen\n",
    "        \n",
    "        filtered_sentence = [] \n",
    "\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "\n",
    "        word_tokens = word_tokenize(example_sent) \n",
    "        \n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "        \n",
    "        return filtered_sentence\n",
    "    # Using \"getting(sen)\" function to append edited sentence to data\n",
    "    x=[]\n",
    "    for i in data[name].values:\n",
    "        x.append(getting(i))\n",
    "    data[name]=x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItIioodcrjso"
   },
   "source": [
    "# Lemmatization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5pL4ylMrjsp"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def Lemmatization(data,name):\n",
    "    def getting2(sen):\n",
    "        \n",
    "        example = sen\n",
    "        output_sentence =[]\n",
    "        word_tokens2 = word_tokenize(example)\n",
    "        lemmatized_output = [lemmatizer.lemmatize(w) for w in word_tokens2]\n",
    "        \n",
    "        # Remove characters which have length less than 2  \n",
    "        without_single_chr = [word for word in lemmatized_output if len(word) > 2]\n",
    "        # Remove numbers\n",
    "        cleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]\n",
    "        \n",
    "        return cleaned_data_title\n",
    "    # Using \"getting2(sen)\" function to append edited sentence to data\n",
    "    x=[]\n",
    "    for i in data[name].values:\n",
    "        x.append(getting2(i))\n",
    "    data[name]=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AklB0al5rjsp"
   },
   "source": [
    "# Converting all the texts back to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkG2Qw3drjsq"
   },
   "outputs": [],
   "source": [
    "def make_sentences(data,name):\n",
    "    data[name]=data[name].apply(lambda x:' '.join([i+' ' for i in x]))\n",
    "    # Removing double spaces if created\n",
    "    data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTcwEmCOrjsq"
   },
   "outputs": [],
   "source": [
    "# Using the preprocessing function to preprocess the hotel data\n",
    "preprocess_Reviews_data(data,'Review_without_stopwords')\n",
    "# Using tokenizer and removing the stopwords\n",
    "rem_stopwords_tokenize(data,'Review_without_stopwords')\n",
    "# Converting all the texts back to sentences\n",
    "make_sentences(data,'Review_without_stopwords')\n",
    "\n",
    "#Edits After Lemmatization\n",
    "final_Edit = data['Review_without_stopwords'].copy()\n",
    "data[\"After_lemmatization\"] = final_Edit\n",
    "\n",
    "# Using the Lemmatization function to lemmatize the hotel data\n",
    "Lemmatization(data,'After_lemmatization')\n",
    "# Converting all the texts back to sentences\n",
    "make_sentences(data,'After_lemmatization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzlyzw-crjsq"
   },
   "source": [
    "# Results of Preprocessing data (Removing stopwords & Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfux6Cnqrjsr",
    "outputId": "3759b51d-f4c7-4568-ea7a-32e013d8a6d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Review_without_stopwords</th>\n",
       "      <th>After_lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1974 teenager martha moxley maggie grace moves...</td>\n",
       "      <td>teenager martha moxley maggie grace move high ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "      <td>ok soreally like kris kristofferson usual easy...</td>\n",
       "      <td>soreally like kris kristofferson usual easy go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "      <td>spoiler read think watching movie although wou...</td>\n",
       "      <td>spoiler read think watching movie although wou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "      <td>hi people seen wonderful movie im sure thet wo...</td>\n",
       "      <td>people seen wonderful movie sure thet would li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "      <td>recently bought dvd forgetting muchhated movie...</td>\n",
       "      <td>recently bought dvd forgetting muchhated movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Leave it to Braik to put on a good show. Final...</td>\n",
       "      <td>1</td>\n",
       "      <td>leave braik put ongood show finally zorak livi...</td>\n",
       "      <td>leave braik put ongood show finally zorak livi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1   \n",
       "1  OK... so... I really like Kris Kristofferson a...          0   \n",
       "2  ***SPOILER*** Do not read this, if you think a...          0   \n",
       "3  hi for all the people who have seen this wonde...          1   \n",
       "4  I recently bought the DVD, forgetting just how...          0   \n",
       "5  Leave it to Braik to put on a good show. Final...          1   \n",
       "\n",
       "                            Review_without_stopwords  \\\n",
       "0  1974 teenager martha moxley maggie grace moves...   \n",
       "1  ok soreally like kris kristofferson usual easy...   \n",
       "2  spoiler read think watching movie although wou...   \n",
       "3  hi people seen wonderful movie im sure thet wo...   \n",
       "4  recently bought dvd forgetting muchhated movie...   \n",
       "5  leave braik put ongood show finally zorak livi...   \n",
       "\n",
       "                                 After_lemmatization  \n",
       "0  teenager martha moxley maggie grace move high ...  \n",
       "1  soreally like kris kristofferson usual easy go...  \n",
       "2  spoiler read think watching movie although wou...  \n",
       "3  people seen wonderful movie sure thet would li...  \n",
       "4  recently bought dvd forgetting muchhated movie...  \n",
       "5  leave braik put ongood show finally zorak livi...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cExcY5vrjsr"
   },
   "source": [
    "So when we are working with sentiwordnet we need to know the characterstic of the word for which we want to know the sentiment . So for finding that position of the word here we are gonna use nltk which tells us about the position of the word which then is used to get the sentiment using the sentiwordnet . We then average out the score for both the positive and the negative score from the whole sentence .\n",
    "The positions compatible with the sentiwordnet are:\n",
    "* n - NOUN\n",
    "* v - VERB\n",
    "* a - ADJECTIVE\n",
    "* s - ADJECTIVE SATELLITE\n",
    "* r - ADVERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9VnC5wxrjsr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ov7c09Vrjss",
    "outputId": "6848e254-6f7a-40ec-b974-85470fdc8489"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       -2.125\n",
      "1        2.375\n",
      "2        1.500\n",
      "3        1.375\n",
      "4       -0.625\n",
      "         ...  \n",
      "49995   -4.500\n",
      "49996    0.625\n",
      "49997   -1.000\n",
      "49998    1.500\n",
      "49999    0.625\n",
      "Name: senti_score, Length: 50000, dtype: float64\n",
      "<bound method NDFrame.head of                                                   review  sentiment  \\\n",
      "0      In 1974, the teenager Martha Moxley (Maggie Gr...          1   \n",
      "1      OK... so... I really like Kris Kristofferson a...          0   \n",
      "2      ***SPOILER*** Do not read this, if you think a...          0   \n",
      "3      hi for all the people who have seen this wonde...          1   \n",
      "4      I recently bought the DVD, forgetting just how...          0   \n",
      "...                                                  ...        ...   \n",
      "49995  OK, lets start with the best. the building. al...          0   \n",
      "49996  The British 'heritage film' industry is out of...          0   \n",
      "49997  I don't even know where to begin on this one. ...          0   \n",
      "49998  Richard Tyler is a little boy who is scared of...          0   \n",
      "49999  I waited long to watch this movie. Also becaus...          1   \n",
      "\n",
      "                                Review_without_stopwords  \\\n",
      "0      1974 teenager martha moxley maggie grace moves...   \n",
      "1      ok soreally like kris kristofferson usual easy...   \n",
      "2      spoiler read think watching movie although wou...   \n",
      "3      hi people seen wonderful movie im sure thet wo...   \n",
      "4      recently bought dvd forgetting muchhated movie...   \n",
      "...                                                  ...   \n",
      "49995  ok lets start best building although hard beli...   \n",
      "49996  british heritage film industry control thereno...   \n",
      "49997  doneven know begin one itall family worst line...   \n",
      "49998  richard tyler islittle boy scared everything d...   \n",
      "49999  waited long watch movie also becauselike bruce...   \n",
      "\n",
      "                                     After_lemmatization  \\\n",
      "0      teenager martha moxley maggie grace move high ...   \n",
      "1      soreally like kris kristofferson usual easy go...   \n",
      "2      spoiler read think watching movie although wou...   \n",
      "3      people seen wonderful movie sure thet would li...   \n",
      "4      recently bought dvd forgetting muchhated movie...   \n",
      "...                                                  ...   \n",
      "49995  let start best building although hard believe ...   \n",
      "49996  british heritage film industry control thereno...   \n",
      "49997  doneven know begin one itall family worst line...   \n",
      "49998  richard tyler islittle boy scared everything d...   \n",
      "49999  waited long watch movie also becauselike bruce...   \n",
      "\n",
      "                                                pos_tags  senti_score  \n",
      "0      [(teenager, NN), (martha, NN), (moxley, NN), (...       -2.125  \n",
      "1      [(soreally, RB), (like, IN), (kris, NNP), (kri...        2.375  \n",
      "2      [(spoiler, NN), (read, NN), (think, VBP), (wat...        1.500  \n",
      "3      [(people, NNS), (seen, VBN), (wonderful, JJ), ...        1.375  \n",
      "4      [(recently, RB), (bought, VBD), (dvd, NN), (fo...       -0.625  \n",
      "...                                                  ...          ...  \n",
      "49995  [(let, VB), (start, VB), (best, JJS), (buildin...       -4.500  \n",
      "49996  [(british, JJ), (heritage, NN), (film, NN), (i...        0.625  \n",
      "49997  [(doneven, RB), (know, VBP), (begin, VB), (one...       -1.000  \n",
      "49998  [(richard, RB), (tyler, NN), (islittle, JJ), (...        1.500  \n",
      "49999  [(waited, VBN), (long, JJ), (watch, NN), (movi...        0.625  \n",
      "\n",
      "[50000 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "pos=neg=obj=count=0\n",
    "\n",
    "postagging = []\n",
    "\n",
    "for review in data['After_lemmatization']:\n",
    "    list = word_tokenize(review)\n",
    "    postagging.append(nltk.pos_tag(list))\n",
    "\n",
    "data['pos_tags'] = postagging\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "# Returns list of pos-neg and objective score. But returns empty list if not present in senti wordnet.\n",
    "def get_sentiment(word,tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    \n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "\n",
    "    #Lemmatization\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "\n",
    "    #Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. \n",
    "    #Synset instances are the groupings of synonymous words that express the same concept. \n",
    "    #Some of the words have only one Synset and some have several.\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "\n",
    "    # Take the first sense, the most common\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "\n",
    "    return [synset.name(), swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
    "\n",
    "    pos=neg=obj=count=0\n",
    "    \n",
    "    ###################################################################################\n",
    "senti_score = []\n",
    "\n",
    "for pos_val in data['pos_tags']:\n",
    "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
    "    for score in senti_val:\n",
    "        try:\n",
    "            pos = pos + score[1]  #positive score is stored at 2nd position\n",
    "            neg = neg + score[2]  #negative score is stored at 3rd position\n",
    "        except:\n",
    "            continue\n",
    "    senti_score.append(pos - neg)\n",
    "    pos=neg=0    \n",
    "    \n",
    "data['senti_score'] = senti_score\n",
    "print(data['senti_score'])\n",
    "\n",
    "print(data.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwGWsgJprjss"
   },
   "outputs": [],
   "source": [
    "overall=[]\n",
    "for i in range(len(data)):\n",
    "    if data['senti_score'][i]>= 0.05:\n",
    "        overall.append('Positive')\n",
    "    elif data['senti_score'][i]<= -0.05:\n",
    "        overall.append('Negative')\n",
    "    else:\n",
    "        overall.append('Neutral')\n",
    "data['Overall Sentiment']=overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enL-Ajdzrjss",
    "outputId": "90e6e827-7095-4736-e348-43fe50671ecf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Review_without_stopwords</th>\n",
       "      <th>After_lemmatization</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>senti_score</th>\n",
       "      <th>Overall Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's been about 14 years since Sharon Stone aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>itbeen 14 years since sharon stone awarded vie...</td>\n",
       "      <td>itbeen year since sharon stone awarded viewers...</td>\n",
       "      <td>[(itbeen, JJ), (year, NN), (since, IN), (sharo...</td>\n",
       "      <td>1.944</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>someone needed to make a car payment... this i...</td>\n",
       "      <td>0</td>\n",
       "      <td>someone needed makecar payment truly awful mak...</td>\n",
       "      <td>someone needed makecar payment truly awful mak...</td>\n",
       "      <td>[(someone, NN), (needed, VBD), (makecar, NN), ...</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Guidelines state that a comment must conta...</td>\n",
       "      <td>0</td>\n",
       "      <td>guidelines state thatcomment must containminim...</td>\n",
       "      <td>guideline state thatcomment must containminimu...</td>\n",
       "      <td>[(guideline, NN), (state, NN), (thatcomment, N...</td>\n",
       "      <td>1.500</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie is a muddled mish-mash of clichés f...</td>\n",
       "      <td>0</td>\n",
       "      <td>movie ismuddled mish mash clichés recent cinem...</td>\n",
       "      <td>movie ismuddled mish mash clichés recent cinem...</td>\n",
       "      <td>[(movie, NN), (ismuddled, VBD), (mish, JJ), (m...</td>\n",
       "      <td>-4.875</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Before Stan Laurel became the smaller half of ...</td>\n",
       "      <td>0</td>\n",
       "      <td>stan laurel became smaller half time greatest ...</td>\n",
       "      <td>stan laurel became smaller half time greatest ...</td>\n",
       "      <td>[(stan, JJ), (laurel, NN), (became, VBD), (sma...</td>\n",
       "      <td>3.875</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>This is the best movie I've ever seen! &lt;br /&gt;&lt;...</td>\n",
       "      <td>1</td>\n",
       "      <td>best movieve ever seen br br maybe itbecauseli...</td>\n",
       "      <td>best movieve ever seen maybe itbecauselive jus...</td>\n",
       "      <td>[(best, JJS), (movieve, NN), (ever, RB), (seen...</td>\n",
       "      <td>0.250</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The morbid Catholic writer Gerard Reve (Jeroen...</td>\n",
       "      <td>1</td>\n",
       "      <td>morbid catholic writer gerard reve jeroen krab...</td>\n",
       "      <td>morbid catholic writer gerard reve jeroen krab...</td>\n",
       "      <td>[(morbid, NN), (catholic, JJ), (writer, NN), (...</td>\n",
       "      <td>1.875</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Semana Santa\" or \"Angel Of Death\" is a very w...</td>\n",
       "      <td>0</td>\n",
       "      <td>semana santa angel death isvery weak movie mir...</td>\n",
       "      <td>semana santa angel death isvery weak movie mir...</td>\n",
       "      <td>[(semana, NN), (santa, NN), (angel, VBZ), (dea...</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Somebody mastered the difficult task of mergin...</td>\n",
       "      <td>1</td>\n",
       "      <td>somebody mastered difficult task merging sport...</td>\n",
       "      <td>somebody mastered difficult task merging sport...</td>\n",
       "      <td>[(somebody, NN), (mastered, VBD), (difficult, ...</td>\n",
       "      <td>3.375</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Why did I waste 1.5 hours of my life watching ...</td>\n",
       "      <td>0</td>\n",
       "      <td>didwaste 1 5 hours life watching film even mad...</td>\n",
       "      <td>didwaste hour life watching film even made ame...</td>\n",
       "      <td>[(didwaste, NN), (hour, NN), (life, NN), (watc...</td>\n",
       "      <td>1.125</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  It's been about 14 years since Sharon Stone aw...      0   \n",
       "1  someone needed to make a car payment... this i...      0   \n",
       "2  The Guidelines state that a comment must conta...      0   \n",
       "3  This movie is a muddled mish-mash of clichés f...      0   \n",
       "4  Before Stan Laurel became the smaller half of ...      0   \n",
       "5  This is the best movie I've ever seen! <br /><...      1   \n",
       "6  The morbid Catholic writer Gerard Reve (Jeroen...      1   \n",
       "7  \"Semana Santa\" or \"Angel Of Death\" is a very w...      0   \n",
       "8  Somebody mastered the difficult task of mergin...      1   \n",
       "9  Why did I waste 1.5 hours of my life watching ...      0   \n",
       "\n",
       "                            Review_without_stopwords  \\\n",
       "0  itbeen 14 years since sharon stone awarded vie...   \n",
       "1  someone needed makecar payment truly awful mak...   \n",
       "2  guidelines state thatcomment must containminim...   \n",
       "3  movie ismuddled mish mash clichés recent cinem...   \n",
       "4  stan laurel became smaller half time greatest ...   \n",
       "5  best movieve ever seen br br maybe itbecauseli...   \n",
       "6  morbid catholic writer gerard reve jeroen krab...   \n",
       "7  semana santa angel death isvery weak movie mir...   \n",
       "8  somebody mastered difficult task merging sport...   \n",
       "9  didwaste 1 5 hours life watching film even mad...   \n",
       "\n",
       "                                 After_lemmatization  \\\n",
       "0  itbeen year since sharon stone awarded viewers...   \n",
       "1  someone needed makecar payment truly awful mak...   \n",
       "2  guideline state thatcomment must containminimu...   \n",
       "3  movie ismuddled mish mash clichés recent cinem...   \n",
       "4  stan laurel became smaller half time greatest ...   \n",
       "5  best movieve ever seen maybe itbecauselive jus...   \n",
       "6  morbid catholic writer gerard reve jeroen krab...   \n",
       "7  semana santa angel death isvery weak movie mir...   \n",
       "8  somebody mastered difficult task merging sport...   \n",
       "9  didwaste hour life watching film even made ame...   \n",
       "\n",
       "                                            pos_tags  senti_score  \\\n",
       "0  [(itbeen, JJ), (year, NN), (since, IN), (sharo...        1.944   \n",
       "1  [(someone, NN), (needed, VBD), (makecar, NN), ...       -1.250   \n",
       "2  [(guideline, NN), (state, NN), (thatcomment, N...        1.500   \n",
       "3  [(movie, NN), (ismuddled, VBD), (mish, JJ), (m...       -4.875   \n",
       "4  [(stan, JJ), (laurel, NN), (became, VBD), (sma...        3.875   \n",
       "5  [(best, JJS), (movieve, NN), (ever, RB), (seen...        0.250   \n",
       "6  [(morbid, NN), (catholic, JJ), (writer, NN), (...        1.875   \n",
       "7  [(semana, NN), (santa, NN), (angel, VBZ), (dea...       -0.750   \n",
       "8  [(somebody, NN), (mastered, VBD), (difficult, ...        3.375   \n",
       "9  [(didwaste, NN), (hour, NN), (life, NN), (watc...        1.125   \n",
       "\n",
       "  Overall Sentiment  \n",
       "0          Positive  \n",
       "1          Negative  \n",
       "2          Positive  \n",
       "3          Negative  \n",
       "4          Positive  \n",
       "5          Positive  \n",
       "6          Positive  \n",
       "7          Negative  \n",
       "8          Positive  \n",
       "9          Positive  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZdNaxIPrjst",
    "outputId": "b83f137c-4d6d-4559-fcc0-dee3946d5fef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f97a7962550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(data['Overall Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwcPsG7zrjst"
   },
   "outputs": [],
   "source": [
    "data['reviews_text_new'] = data['After_lemmatization'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2qZstFbrjsu"
   },
   "source": [
    "# Building a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_KPE47Zrjsu"
   },
   "source": [
    "# Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsKmqD_frjsu",
    "outputId": "15f0d76b-2797-4053-9f64-fe347c227cba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000d</th>\n",
       "      <th>000must</th>\n",
       "      <th>000night</th>\n",
       "      <th>000surly</th>\n",
       "      <th>00m</th>\n",
       "      <th>00schneider</th>\n",
       "      <th>02was</th>\n",
       "      <th>06thsaw</th>\n",
       "      <th>0am</th>\n",
       "      <th>1000on</th>\n",
       "      <th>...</th>\n",
       "      <th>zuthought</th>\n",
       "      <th>zvonimir</th>\n",
       "      <th>zvyagvatsev</th>\n",
       "      <th>zwartboek</th>\n",
       "      <th>zwick</th>\n",
       "      <th>ªsen</th>\n",
       "      <th>álex</th>\n",
       "      <th>ángela</th>\n",
       "      <th>émigré</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 72846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000d  000must  000night  000surly  00m  00schneider  02was  06thsaw  0am  \\\n",
       "0     0        0         0         0    0            0      0        0    0   \n",
       "1     0        0         0         0    0            0      0        0    0   \n",
       "2     0        0         0         0    0            0      0        0    0   \n",
       "\n",
       "   1000on  ...  zuthought  zvonimir  zvyagvatsev  zwartboek  zwick  ªsen  \\\n",
       "0       0  ...          0         0            0          0      0     0   \n",
       "1       0  ...          0         0            0          0      0     0   \n",
       "2       0  ...          0         0            0          0      0     0   \n",
       "\n",
       "   álex  ángela  émigré  über  \n",
       "0     0       0       0     0  \n",
       "1     0       0       0     0  \n",
       "2     0       0       0     0  \n",
       "\n",
       "[3 rows x 72846 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following code creates a word-document matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(data['reviews_text_new'])\n",
    "df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9s1WZz4rjsu",
    "outputId": "bc0a7392-406e-43e3-d608-8d239df998eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1106)\t1\n",
      "  (0, 2588)\t1\n",
      "  (0, 3647)\t1\n",
      "  (0, 4066)\t1\n",
      "  (0, 4142)\t1\n",
      "  (0, 4168)\t1\n",
      "  (0, 4614)\t1\n",
      "  (0, 5463)\t1\n",
      "  (0, 6396)\t1\n",
      "  (0, 6582)\t1\n",
      "  (0, 6952)\t4\n",
      "  (0, 7776)\t1\n",
      "  (0, 9692)\t1\n",
      "  (0, 11859)\t1\n",
      "  (0, 12218)\t1\n",
      "  (0, 12230)\t1\n",
      "  (0, 12825)\t1\n",
      "  (0, 13533)\t1\n",
      "  (0, 14356)\t1\n",
      "  (0, 14636)\t1\n",
      "  (0, 15630)\t1\n",
      "  (0, 15654)\t1\n",
      "  (0, 15898)\t1\n",
      "  (0, 16265)\t3\n",
      "  (0, 17320)\t1\n",
      "  :\t:\n",
      "  (4999, 46844)\t1\n",
      "  (4999, 49261)\t1\n",
      "  (4999, 50379)\t1\n",
      "  (4999, 51029)\t1\n",
      "  (4999, 51888)\t1\n",
      "  (4999, 52235)\t1\n",
      "  (4999, 52798)\t1\n",
      "  (4999, 53329)\t1\n",
      "  (4999, 53389)\t1\n",
      "  (4999, 54860)\t1\n",
      "  (4999, 56474)\t1\n",
      "  (4999, 57725)\t1\n",
      "  (4999, 58166)\t1\n",
      "  (4999, 58795)\t1\n",
      "  (4999, 58858)\t1\n",
      "  (4999, 61963)\t1\n",
      "  (4999, 62069)\t1\n",
      "  (4999, 63242)\t1\n",
      "  (4999, 65801)\t1\n",
      "  (4999, 66033)\t1\n",
      "  (4999, 66094)\t1\n",
      "  (4999, 70410)\t1\n",
      "  (4999, 71498)\t1\n",
      "  (4999, 71556)\t1\n",
      "  (4999, 72562)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(data['reviews_text_new'])\n",
    "vect.get_feature_names()\n",
    "# transform training data into a 'document-term matrix'\n",
    "simple_train_dtm = vect.transform(data['reviews_text_new'])\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjkQeuhbrjsv"
   },
   "outputs": [],
   "source": [
    "### Creating a python object of the class CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_counts = CountVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
    "                             ngram_range=(1,3)) # number of n-grams\n",
    "\n",
    "bow_data = bow_counts.fit_transform(data['reviews_text_new'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTXHuHwPrjsv"
   },
   "source": [
    "# Divide into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0iXRu2Lrjsv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_data, # Features\n",
    "                                                                    data['Overall Sentiment'], # Target variable\n",
    "                                                                    test_size = 0.2, # 20% test size\n",
    "                                                                    random_state = 0) # random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOWYiSQQrjsv"
   },
   "source": [
    "# Applying logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1g1czQVrjsv",
    "outputId": "c2bc354e-dd1e-4503-a3ef-1cfc275e8778"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.73      0.77      2955\n",
      "     Neutral       0.75      0.01      0.03       213\n",
      "    Positive       0.87      0.94      0.91      6832\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.81      0.56      0.57     10000\n",
      "weighted avg       0.85      0.86      0.85     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "### Training the model \n",
    "lr_model_all = LogisticRegression() # Logistic regression\n",
    "lr_model_all.fit(X_train_bow, y_train_bow) # Fitting a logistic regression model\n",
    "\n",
    "## Predicting the output\n",
    "test_pred_lr_all = lr_model_all.predict(X_test_bow) # Class prediction\n",
    "\n",
    "\n",
    "## Calculate key performance metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "# Print a classification report\n",
    "print(classification_report(y_test_bow,test_pred_lr_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdDURKX7rjsw"
   },
   "source": [
    "# TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLBhyEn7rjsw"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "noise_words = []\n",
    "### Creating a python object of the class CountVectorizer\n",
    "tfidf_counts = TfidfVectorizer(tokenizer= word_tokenize, # type of tokenization\n",
    "                               stop_words=noise_words, # List of stopwords\n",
    "                               ngram_range=(1,1)) # number of n-grams\n",
    "\n",
    "tfidf_data = tfidf_counts.fit_transform(data['reviews_text_new'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fevp6xjrjsw"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(tfidf_data,\n",
    "                                                                            data['Overall Sentiment'],\n",
    "                                                                            test_size = 0.2,\n",
    "                                                                            random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJ8G8o04rjsw",
    "outputId": "de66926d-5c6a-41fa-ea50-4b6ef8c4308d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning:\n",
      "\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.67      0.75      2955\n",
      "     Neutral       0.00      0.00      0.00       213\n",
      "    Positive       0.85      0.95      0.90      6832\n",
      "\n",
      "    accuracy                           0.85     10000\n",
      "   macro avg       0.56      0.54      0.55     10000\n",
      "weighted avg       0.83      0.85      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Setting up the model class\n",
    "lr_model_tf_idf = LogisticRegression()\n",
    "\n",
    "## Training the model \n",
    "lr_model_tf_idf.fit(X_train_tfidf,y_train_tfidf)\n",
    "\n",
    "## Prediciting the results\n",
    "test_pred_lr_all = lr_model_tf_idf.predict(X_test_tfidf)\n",
    "\n",
    "## Calculate key performance metrics\n",
    "\n",
    "\n",
    "# Print a classification report\n",
    "print(classification_report(y_test_tfidf,test_pred_lr_all))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
