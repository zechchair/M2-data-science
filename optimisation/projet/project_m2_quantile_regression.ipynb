{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT: Course Optimization for Data Science 2022-2023\n",
    "## Optimization strategies for the \"smoothed\" quantile regression with (non-)smooth penalties\n",
    "\n",
    "Author: Alexandre Gramfort\n",
    "\n",
    "If you have questions or if something is not clear in the text below please contact us\n",
    "by email.\n",
    "\n",
    "## Aim:\n",
    "\n",
    "- Derive mathematically and implement the loss and gradient of a smoothed pinball loss used for quantile regression\n",
    "- Implement your own solvers for L1 and L2 regularizations with: (Accelerated) Proximal gradient descent, proximal coordinate descent and L-BFGS (only for L2)\n",
    "- Implement your own scikit-learn estimator for L1 or L2 regularized smoothed-quantile regression model and test it on a real dataset where we will look at prediction invtervals.\n",
    "\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "This work must be done by pairs of students.\n",
    "Each student must send their work before the 2nd of January 2023 at 23:59, using the moodle platform.\n",
    "This means that **each student in the pair sends the same file**\n",
    "\n",
    "On the moodle, in the \"Optimization for Data Science\" course, you have a \"devoir\" section called \"Project\".\n",
    "This is where you submit your jupyter notebook file.\n",
    "\n",
    "The name of the file must be constructed as in the next cell\n",
    "\n",
    "### Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "#### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"pierre\"\n",
    "ln1 = \"ablin\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"project\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Smoothed quantile regression model\n",
    "\n",
    "[Quantile regression](https://en.wikipedia.org/wiki/Quantile_regression) is a supervised learning regression model with target space $\\mathcal{Y} = \\mathbb{R}$. It is a method widely used in statistics to obtain prediction intervals, and it is very often used in time series forecasting.\n",
    "\n",
    "We consider observations $x \\in \\mathbb{R}^{P}$. $P$ is the number of features, and we will denote the $N$ observations available in the training set as $x_1, \\dots, x_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a linear model, _i.e.,_ we want to learn the parameters $\\theta = (w, b) \\in \\mathbb{R}^{P}\\times \\mathbb{R}$ such that the value of $w^\\top x + b$ is the $\\tau$-quantile of the conditional distribution $y | x$. For example, for $\\tau=0.5$, we want to learn the parameters such that the value of $w^\\top x + b$ is the *median* of the conditional distribution $y | x$. For $\\tau=0.9$, we want to learn the parameters such that the value of $w^\\top x + b$ is the *90th percentile* of the conditional distribution $y | x$, meaning that 90% of the observations of $y | x$ are below $w^\\top x + b$.\n",
    "\n",
    "An estimate $\\hat{q}$ of the $\\tau$-conditional sample quantile is obtained by minimizing the following loss function:\n",
    "$$\n",
    "\\hat{q} = \\arg\\min_{q}\n",
    "    \\frac{1}{N} \\sum_{i=1}^N\n",
    "    \\left[\n",
    "            (\\tau - 1) \\mathbb{1}_{\\{y_i < q\\}} (y_i - q) + \n",
    "            \\tau \\mathbb{1}_{\\{y_i \\geq q\\}} (y_i - q)\n",
    "    \\right]\n",
    "$$\n",
    "\n",
    "The function $\\ell_\\tau(z) = (\\tau - 1) \\mathbb{1}_{\\{z \\leq 0\\}} (z) + \\tau \\mathbb{1}_{\\{z \\geq 0\\}} (z)$ is known as the **pinball loss**. The above loss can then be rewritten as:\n",
    "$$\n",
    "\\hat{q} = \\arg\\min_{q}\n",
    "    \\frac{1}{N} \\sum_{i=1}^N \\ell_\\tau(y_i - q) \\enspace.\n",
    "$$\n",
    "The optimization problem becomes:\n",
    "$$\n",
    "\\hat{w}, \\hat{b} = \\arg\\min_{w, b} L(w, b)\n",
    "    \\enspace \\mathrm{with} \\enspace L(w, b) = \\frac{1}{N} \\sum_{i=1}^N \\ell_\\tau(y_i - w^\\top x_i - b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing some regularization on the parameter $w$ with parameter $\\lambda \\ge 0$, the penalized estimation problem is written:\n",
    "\n",
    "$$\n",
    "    (\\mathcal{P}^{\\mathcal{R}}):\n",
    "\t\\min_{w \\in \\mathbb{R}^P, b \\in \\mathbb{R}} \\quad L(w, b) + \\lambda \\mathcal{R}(w)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{R}(w)$ can be :\n",
    "- $\\mathcal{R}_1(w) = \\|w\\|_1 = \\sum_{j=1}^P |w_{j}|$, ie. an $\\ell_1$ norm\n",
    "- $\\mathcal{R}_2(w) = \\|w\\|_2^2 = \\sum_{j=1}^P w_{j}^2$, ie. a squared $\\ell_2$ norm\n",
    "\n",
    "Here is an implementation of the pinball loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinball(z, tau=0.5):\n",
    "    \"\"\"Compute the pinball loss for z and a quantile tau.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : ndarray, shape (n_samples,)\n",
    "        The values on which to compute the pinball loss.\n",
    "    tau : float in [0, 1]\n",
    "        The quantile.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss : ndarray, shape (n_samples,)\n",
    "        The value of the pinball loss for each value of z.\n",
    "    \"\"\"\n",
    "    sign = (z >= 0).astype(z.dtype)\n",
    "    loss = tau * sign * z - (1 - tau) * (1 - sign) * z\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 0\n",
    "\n",
    "- Plot the pinball loss for tau=0.3, 0.5 and 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 1:\n",
    "\n",
    "- Justify that $(\\mathcal{P}^{\\mathcal{R}_1})$ and $(\\mathcal{P}^{\\mathcal{R}_2})$ are convex optimization problems.\n",
    "- Can you readily apply gradient descent or proximal gradient descent to solve these two problems? You will justify your answer by commenting on the smoothness of the objective functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate what type of solution one obtains when minimizing the pinball loss. For now, we will use a black-box optimization method from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = np.linspace(start=0, stop=10, num=100)\n",
    "X = x[:, np.newaxis]  # make X two-dimensional with one column\n",
    "y_true = 10 + 0.5 * x\n",
    "y = y_true + rng.normal(loc=0, scale=0.5 + 0.5 * x, size=x.shape[0])\n",
    "\n",
    "plt.plot(x, y_true, \"s-\", label='Groundtruth', markevery=10)\n",
    "plt.plot(x, y, 'o', label=\"Data\")\n",
    "tau = 0.3\n",
    "\n",
    "def pobj(wb, tau):\n",
    "    w, b = wb[:1], wb[-1]\n",
    "    return np.mean(pinball(y - X @ w - b, tau=tau))\n",
    "\n",
    "for tau in [0.1, 0.5, 0.9]:\n",
    "    w_hat, b_hat = minimize(lambda wb : pobj(wb, tau), x0=[0, 0]).x\n",
    "    plt.plot(x, x * w_hat + b_hat, label=f\"tau={tau}\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can observe that the predictions obtained with 0.1 and 0.9 form an interval that can be used to evaluate the uncertainty of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothed pinball loss\n",
    "\n",
    "Let us define the function:\n",
    "\n",
    "$$\n",
    "    \\ell_\\tau^\\delta(z) = \\min_{u \\in \\mathbb{R}} \\quad\n",
    "    \\underbrace{\n",
    "        \\ell_\\tau(u) + \\tfrac{1}{2\\delta} (z - u)^2\n",
    "    }_{J(u)}\n",
    "$$\n",
    "\n",
    "#### QUESTION 2:\n",
    "- Justify that the function $\\ell_\\tau^\\delta: z \\rightarrow \\ell_\\tau^\\delta(z)$ is well defined, meaning that a unique function value exists for each value of z.\n",
    "- Prove that for any $\\delta >0$, we have $\\ell_\\tau^\\delta(z) \\leq \\ell_\\tau(z)$.\n",
    "- Prove that the function $\\ell_\\tau^\\delta$ is given by the formula just below:\n",
    "\n",
    "$$\n",
    "    \\ell_\\tau^\\delta (z) = \\left\\{\n",
    "\t\\begin{aligned}\n",
    "\t\\tau (z - \\frac{\\delta \\tau}{2}) & \\quad \\mathrm{ if } \\quad  \\delta \\tau \\leq z \\\\\n",
    "\t\\frac{z^2}{2 \\delta} & \\quad \\mathrm{ if } \\quad 0 \\leq z < \\tau \\delta \\\\\n",
    "\t\\frac{z^2}{2 \\delta} & \\quad \\mathrm{ if } \\quad  -\\delta (1 - \\tau) \\leq z < 0 \\\\\n",
    "    (1-\\tau) \\left( -z - \\tfrac{\\delta}{2} (1 - \\tau) \\right) & \\quad \\mathrm{ otherwise }\n",
    "\t\\end{aligned}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 3:\n",
    "- Implement the function `smooth_pinball(z, tau, delta)` that applies $\\ell_\\tau^\\delta$ to each entry a numpy array. You will plot the function `smooth_pinball` and `pinball` between -2 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_pinball(z, tau, delta):\n",
    "    \"\"\"Compute the smoothed pinball loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array-like, shape (n_samples,)\n",
    "        The array on which to compute the smoothed pinball loss.\n",
    "    tau : float in [0, 1]\n",
    "        The quantile.\n",
    "    delta : float\n",
    "        The smoothing parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    l : array-like, shape (n_samples,)\n",
    "        The smoothed pinball loss values.\n",
    "    \"\"\"\n",
    "    out = z.copy().astype(np.float64)\n",
    "    ### TODO\n",
    "\n",
    "    ### END TODO\n",
    "    return out\n",
    "\n",
    "### TODO plotting\n",
    "\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 4:\n",
    "- What is the derivative of the smoothed pinball loss $\\ell_\\tau^\\delta$?\n",
    "- Justify that the derivative of the smoothed pinball loss is continuous.\n",
    "- Justify that the function $\\ell_\\tau^\\delta$ is L-smooth. Propose a value for the Lipschitz constant of its gradient.\n",
    "- Justify that the function $\\ell_\\tau^\\delta$ is convex. Is it strongly convex?\n",
    "- Implement the function `grad_smooth_pinball(z, tau, delta)` that applies the derivative of $\\ell_\\tau^\\delta$ to each entry of a numpy array.\n",
    "\n",
    "**Remark:** You will use the `scipy.optimize.check_grad` function to assess the validity of your result. You will need to test your gradient in both the linear and quadratic regions of the function (not just in one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_smooth_pinball(z, tau, delta):\n",
    "    \"\"\"Compute the derivative of the smoothed pinball loss\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array-like, shape (n_samples,)\n",
    "        The array on which to compute the smoothed pinball loss.\n",
    "    tau : float in [0, 1]\n",
    "        The quantile.\n",
    "    delta : float\n",
    "        The smoothing parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dl : array-like, shape (n_samples,)\n",
    "        The derivative of smoothed pinball loss computed element-wise.\n",
    "    \"\"\"\n",
    "    out = z.copy().astype(np.float64)\n",
    "    ### TODO\n",
    "\n",
    "    ### END TODO\n",
    "    return out\n",
    "\n",
    "x = np.linspace(-2, 2, 100)\n",
    "tau = 0.3\n",
    "plt.plot(x, grad_smooth_pinball(x, tau=tau, delta=.5), label='Gradient of Smoothed Pinball loss')\n",
    "plt.legend();\n",
    "\n",
    "from scipy.optimize import check_grad\n",
    "\n",
    "### TODO check gradient with check_grad function.\n",
    "\n",
    "### END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we consider the following optimization problem:\n",
    "\n",
    "$$\n",
    "    (\\mathcal{P}_\\delta^\\mathcal{R}): \\min_{w, b} \\quad L_\\delta(w, b) + \\lambda \\mathcal{R}(w)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{R}(w)$ can be $\\mathcal{R}_1(w)$ or $\\mathcal{R}_2(w)$, and where $L_\\delta(w, b) = \\frac{1}{N} \\sum_{i=1}^N \\ell_\\tau^\\delta(y_i - w^\\top x_i - b)$.\n",
    "\n",
    "#### QUESTION 5:\n",
    "- Justify what optimization strategy among L-BFGS, (proximal-)gradient descent, (proximal-)coordinate descent is readily applicable\n",
    "  for $(\\mathcal{P}_\\delta^{\\mathcal{R}_2})$ and $(\\mathcal{P}_\\delta^{\\mathcal{R}_1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 6:\n",
    "- Solve the optimization prolem $(\\mathcal{P}_\\delta^{\\mathcal{R}_2})$ using the `fmin_l_bfgs_b` function from `scipy.optimize`. You are expected to provide the explicit gradient (fprime parameter) to `fmin_l_bfgs_b`.\n",
    "- Using the simulated dataset from above, you will check that your solver gives comparable results as obtained above without smoothing when setting a very small value for $\\lambda$. Your are expected to make a plot of the regression fit.\n",
    "\n",
    "The estimate of $w$ and $b$ should be called `w_hat` and `b_hat`. You will call the regularization parameter $\\lambda$ as `lbda` in the code.\n",
    "\n",
    "To help you, we provide you with the function `pobj_l2` that computes the objective to minimize. Note that the parameters `w` and `b` are combined in a single array `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "lbda = 0.01\n",
    "delta = 0.5\n",
    "\n",
    "def pobj_l2(params, X=X, y=y, lbda=lbda, tau=tau, delta=delta):\n",
    "    \"\"\"Objective function with L2 penalty.\"\"\"\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    residual = y - np.dot(X, w) - b\n",
    "    return np.mean(smooth_pinball(residual, tau=tau, delta=delta)) + lbda * np.sum(w ** 2)\n",
    "\n",
    "\n",
    "def smooth_pinball_lbfgs_l2(X=X, y=y, lbda=lbda, tau=tau, delta=delta):\n",
    "    \"\"\"Estimate w and b using L-BFGS.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape (n_samples, n_features)\n",
    "        The array of features.\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        The target.\n",
    "    lbda : float\n",
    "        The regularization parameter lbda\n",
    "    tau : float in [0, 1]\n",
    "        The quantile.\n",
    "    delta : float\n",
    "        The smoothing parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w_hat : ndarray, shape (n_features,)\n",
    "        The estimated w.\n",
    "    b_hat : float\n",
    "        The estimated b.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return w_hat, b_hat\n",
    "\n",
    "\n",
    "w_hat, b_hat = smooth_pinball_lbfgs_l2(X, y, lbda=0.01, tau=0.3, delta=0.5)\n",
    "y_pred = np.dot(X, w_hat) + b_hat\n",
    "\n",
    "# TODO  (visualization of the fit)\n",
    "\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Smooth quantile regression with L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are interested in the $\\ell_1$ regularized model.\n",
    "To help you we give you the code of the objective function to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pobj_l1(params, X=X, y=y, lbda=lbda, tau=tau, delta=delta):\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    return np.mean(smooth_pinball(y - np.dot(X, w) - b, tau=tau, delta=delta)) + lbda * np.sum(np.abs(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the cost function, you are going to implement solvers based on:\n",
    "\n",
    "- Proximal Gradient Descent (PGD aka ISTA)\n",
    "- Accelerated Proximal Gradient Descent (APGD aka FISTA)\n",
    "- Proximal Coordinate Descent (PCD)\n",
    "\n",
    "Before this we are going to define the `monitor` class previously used in the second lab as well as plotting functions useful to monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class monitor(object):\n",
    "    def __init__(self, algo, obj, x_min, args=()):\n",
    "        self.x_min = x_min\n",
    "        self.algo = algo\n",
    "        self.obj = obj\n",
    "        self.args = args\n",
    "        if self.x_min is not None:\n",
    "            self.f_min = obj(x_min, *args)\n",
    "\n",
    "    def run(self, *algo_args, **algo_kwargs):\n",
    "        t0 = time.time()\n",
    "        _, x_list = self.algo(*algo_args, **algo_kwargs)\n",
    "        self.total_time = time.time() - t0\n",
    "        self.x_list = x_list\n",
    "        if self.x_min is not None:\n",
    "            self.err = [linalg.norm(x - self.x_min) for x in x_list]\n",
    "            self.obj = [self.obj(x, *self.args) - self.f_min for x in x_list]\n",
    "        else:\n",
    "            self.obj = [self.obj(x, *self.args) for x in x_list]\n",
    "\n",
    "\n",
    "def plot_epochs(monitors, solvers):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    for monit in monitors:\n",
    "        ax1.semilogy(monit.obj, lw=2)\n",
    "        ax1.set_title(\"Objective\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        if monit.x_min is None:\n",
    "            ax1.set_ylabel(\"$f(x_k)$\")\n",
    "        else:\n",
    "            ax1.set_ylabel(\"$f(x_k) - f(x^*)$\")\n",
    "\n",
    "    ax1.legend(solvers)\n",
    "\n",
    "    for monit in monitors:\n",
    "        if monit.x_min is not None:\n",
    "            ax2.semilogy(monit.err, lw=2)\n",
    "            ax2.set_title(\"Distance to optimum\")\n",
    "            ax2.set_xlabel(\"Epoch\")\n",
    "            ax2.set_ylabel(\"$\\|x_k - x^*\\|_2$\")\n",
    "\n",
    "    ax2.legend(solvers)\n",
    "\n",
    "\n",
    "def plot_time(monitors, solvers):\n",
    "    for monit in monitors:\n",
    "        objs = monit.obj\n",
    "        plt.semilogy(np.linspace(0, monit.total_time, len(objs)), objs, lw=2)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.xlabel(\"Timing\")\n",
    "        plt.ylabel(\"$f(x_k) - f(x^*)$\")\n",
    "\n",
    "    plt.legend(solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 7a:\n",
    "- Implement the proximal gradient descent (PGD) method.\n",
    "\n",
    "**Note:**  The parameter `step` is the size of the gradient step that you will need to propose by computing the Lipschitz constant of the data fitting term (Smooth pinball term without regularization term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd(x_init, grad, prox, step, n_iter=100, store_every=1,\n",
    "        grad_args=(), prox_args=()):\n",
    "    \"\"\"Proximal gradient descent algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_init : array, shape (n_parameters,)\n",
    "        Parameters of the optimization problem.\n",
    "    grad : callable\n",
    "        The gradient of the smooth data fitting term.\n",
    "    prox : callable\n",
    "        The proximal operator of the regularization term.\n",
    "    step : float\n",
    "        The size of the gradient step done on the smooth term.\n",
    "    n_iter : int\n",
    "        The number of iterations.\n",
    "    store_every : int\n",
    "        At which frequency should the current iterated be remembered.\n",
    "    grad_args : tuple\n",
    "        Parameters to pass to grad.\n",
    "    prox_args : tuple\n",
    "        Parameters to pass to prox.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : array, shape (n_parameters,)\n",
    "        The estimated parameters.\n",
    "    x_list : list\n",
    "        The list if x values along the iterations.\n",
    "    \"\"\"\n",
    "    x = x_init.copy()\n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 7b:</b>\n",
    "- Implement the L1 and L2 proximal operators. You will pay attention to the intercept.\n",
    "- Using the monitor class and the plot_epochs function, display the convergence.\n",
    "\n",
    "In order to get a good value of `x_min` you will let your PGD solver run for 10000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will need to implement the proximal operator functions for $\\ell_1$ and $\\ell_2$ regularized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_R2(params, reg=1.):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "\n",
    "def prox_R1(params, reg=1.):\n",
    "    # TODO\n",
    "\n",
    "    # END TODO\n",
    "    return params\n",
    "\n",
    "\n",
    "def prox_l2(params, step, lbda):\n",
    "    return prox_R2(params, reg=step * lbda)\n",
    "\n",
    "\n",
    "def prox_l1(params, step, lbda):\n",
    "    return prox_R1(params, reg=step * lbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate bigger data\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=100, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Set initial values of parameters to optimize\n",
    "x_init = np.zeros(n_features + 1)\n",
    "x_init[0] = np.mean(y)\n",
    "n_iter = 10_000\n",
    "lbda = 1\n",
    "delta = 0.5\n",
    "tau = 0.5\n",
    "\n",
    "# TODO (implement function grad_smooth_pinball_loss and define step)\n",
    "\n",
    "# END TODO\n",
    "\n",
    "# Get a good x_min by letting the algorithm converge with many iterations\n",
    "x_min, _ = pgd(x_init, grad_smooth_pinball_loss, prox_l2, step, n_iter=10000, store_every=1000,\n",
    "               grad_args=(X, y, tau, delta), prox_args=(lbda,))\n",
    "\n",
    "# Run PGD\n",
    "monitor_pgd_l2 = monitor(pgd, pobj_l2, x_min, args=(X, y, lbda, tau, delta))\n",
    "monitor_pgd_l2.run(x_init, grad_smooth_pinball_loss, prox_l2, step, n_iter,\n",
    "                   grad_args=(X, y, tau, delta), prox_args=(lbda,))\n",
    "\n",
    "monitors = [monitor_pgd_l2]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the $\\ell_1$ regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PGD for L1\n",
    "\n",
    "x_min_l1, _ = pgd(x_init, grad_smooth_pinball_loss, prox_l1, step, n_iter=10000, store_every=1000,\n",
    "               grad_args=(X, y, tau, delta), prox_args=(lbda,))\n",
    "monitor_pgd_l1 = monitor(pgd, pobj_l1, x_min=x_min_l1, args=(X, y, lbda, tau, delta))\n",
    "monitor_pgd_l1.run(x_init, grad_smooth_pinball_loss, prox_l1, step, n_iter,\n",
    "                   grad_args=(X, y, tau, delta), prox_args=(lbda,))\n",
    "\n",
    "monitors = [monitor_pgd_l1]\n",
    "solvers = [\"PGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUESTION 8:\n",
    "- Implement the accelerated proximal gradient descent (APGD) and add this solver to the monitoring plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apgd(x_init, grad, prox, step, n_iter=100, store_every=1,\n",
    "         grad_args=(), prox_args=()):\n",
    "    \"\"\"Accelerated proximal gradient descent algorithm.\"\"\"\n",
    "    x = x_init.copy()\n",
    "    y = x_init.copy()\n",
    "    t = 1.\n",
    "    x_list = []\n",
    "    for i in range(n_iter):\n",
    "        ### TODO\n",
    "\n",
    "        ### END TODO\n",
    "        if i % store_every == 0:\n",
    "            x_list.append(x.copy())\n",
    "    return x, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l2, monitor_apgd_l2]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# END TODO\n",
    "\n",
    "monitors = [monitor_pgd_l1, monitor_apgd_l1]\n",
    "solvers = [\"PGD\", \"APGD\"]\n",
    "plot_epochs(monitors, solvers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 9\n",
    "- Implement the proximal coordinate descent (PCD) and add this solver to the monitoring plots for L1 and L2 regularized models.\n",
    "\n",
    "**Note:** You are welcome to try to use numba to get reasonable performance but don't spend too much time if you get weird numba errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTION 10\n",
    "- Compare the performance of the different solvers for different (simulated) problem sizes. You will test on a dataset with n_samples >> n_features, n_samples << n_features, and for high and low values of $\\lambda$.\n",
    "- What solver would you recommend for each of the 4 scenarios?\n",
    "\n",
    "**Remark:** What is expected from you here is to provide clear and synthetic plots that show the performance of the different solvers for the different simulated datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Application\n",
    "\n",
    "You will now apply your solver to some census dataset where we aim to predict the wage of a person give some demographic and work related variables.\n",
    "\n",
    "**Disclaimer:** This dataset is not huge and regularization makes little sense with so little features but it serves as a simple illustration.\n",
    "\n",
    "Let's first inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "wages = fetch_openml(name=\"cps_85_wages\", as_frame=True)\n",
    "X_df, y_df = wages[\"data\"], wages[\"target\"]\n",
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's extract `X` taking only the numerical columns (y is already defined above) and do some basic plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_num = X_df.select_dtypes(include=(float, int))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 4))\n",
    "for x, col_name, ax in zip(X_df_num.values.T, X_df.columns, axes.ravel()):\n",
    "    ax.plot(x, y_df, 'o')\n",
    "    ax.set_title(col_name)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_df_num.values\n",
    "y = y_df.values\n",
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now X and y!\n",
    "\n",
    "In order to facilitate our experiment we're going to write a full scikit-learn estimator.\n",
    "\n",
    "#### QUESTION 11\n",
    "- Implement the `fit` method from the estimator in the next cell and fit the model with the different solvers you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "class SmoothQuantileRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"scikit-learn estimator for regression with a smoothed quantile loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lbda : float\n",
    "        The regularization parameter\n",
    "    penalty : 'l1' | 'l2'\n",
    "        The type of regularization to use.\n",
    "    tau : float in (0, 1)\n",
    "        The quantile to predict.\n",
    "    delta : float\n",
    "        The smoothing parameter.\n",
    "    max_iter : int\n",
    "        The number of iterations / epochs to do on the data.\n",
    "    solver : 'pgd' | 'apgd' | 'pcd'\n",
    "        The type of algorithm to use.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    coef_ : ndarray, (n_features,)\n",
    "        The weitghs w.\n",
    "    intercept_ : float\n",
    "        The intercept or bias term b.\n",
    "    \"\"\"\n",
    "    def __init__(self, lbda=1., penalty='l2', tau=0.5, delta=0.5,\n",
    "                 max_iter=2000, solver='pgd'):\n",
    "        self.lbda = lbda\n",
    "        self.penalty = penalty\n",
    "        self.tau = tau\n",
    "        self.delta = delta\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "        assert self.penalty in ['l1', 'l2']\n",
    "        assert self.solver in ['pgd', 'apgd', 'pcd'] \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The target.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO\n",
    "    \n",
    "        # END TODO\n",
    "        self.params_ = x\n",
    "        self.coef_ = x[1:]\n",
    "        self.intercept_ = x[0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray, shape (n_samples,)\n",
    "            The predicted target.\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.coef_) + self.intercept_\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Score using the pinball loss.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            The features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The target.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            The negative pinball loss.\n",
    "            Negative to keep the semantic that higher is better.\n",
    "        \"\"\"\n",
    "        return -np.mean(pinball(y - self.predict(X), tau=self.tau))\n",
    "\n",
    "lbda = 1e-4\n",
    "max_iter = 1000\n",
    "\n",
    "X_scaled = scale(X)  # standardize the features\n",
    "\n",
    "for solver in ['pgd', 'apgd', 'pcd']:\n",
    "    clf = SmoothQuantileRegressor(lbda=lbda, penalty='l2', max_iter=max_iter, solver=solver)\n",
    "    clf.fit(X_scaled, y)\n",
    "    print('Solver with L2: %s   \\t-   Pinball : %.5f' % (solver, -clf.score(X, y)))\n",
    "\n",
    "for solver in ['pgd', 'apgd', 'pcd']:\n",
    "    clf = SmoothQuantileRegressor(lbda=lbda, penalty='l1', max_iter=max_iter, solver=solver)\n",
    "    clf.fit(X_scaled, y)\n",
    "    print('Solver with L1: %s   \\t-   Pinball : %.5f' % (solver, -clf.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the predicitons here is a simple plot where you should see the interval of the predictions with 3 different quantiles. Feel free to comment on what you see and go beyond this plot if you feel inspired !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for tau in [0.1, 0.5, 0.9]:\n",
    "    clf = SmoothQuantileRegressor(lbda=lbda, penalty='l2', solver=\"apgd\", tau=tau)\n",
    "    y_pred = clf.fit(X_scaled, y).predict(X_scaled)\n",
    "    ax.plot(y, y_pred, 'o', label=f'tau = {tau:.1f}')\n",
    "    ax.axis(\"square\")\n",
    "\n",
    "ax.set(xlabel=\"y true\", ylabel=\"y pred\", xlim=(0, np.max(y)), ylim=(0, np.max(y)))\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3068153426e781d5497581b972bbd99c396c97fd0bfab71165f11731a6e8bb22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
