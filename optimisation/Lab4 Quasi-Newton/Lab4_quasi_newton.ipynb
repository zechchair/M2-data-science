{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB: quasi-Newton methods\n",
    "\n",
    "Author : Alexandre Gramfort, Jair Montoya, Pierre Ablin\n",
    "\n",
    "The objective of this lab session is to implement:\n",
    "- Newton method\n",
    "- DFP\n",
    "- BFGS\n",
    "- L-BFGS\n",
    "\n",
    "And to investigate their behaviors.\n",
    "\n",
    "You will need to use **line search methods**.\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work as jupyter notebook one week after the lab on the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**.\n",
    "- The **name of the file must be** constructed as in the next cell.\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"pierre\"\n",
    "ln1 = \"ablin\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"tp_newton\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Demo using Gradient descent\n",
    "\n",
    "First import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the necessary function from the optim_utils.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optim_utils import test_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll only need the `test_solver` function.\n",
    "\n",
    "This function expects a function as parameter.\n",
    "\n",
    "The signature of the function `optimizer` to pass should be the following:\n",
    "\n",
    "`optimizer(x0, f, f_grad, f_hessian)`\n",
    "\n",
    "First, an example with a gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x0, f, f_grad, f_hessian=None):\n",
    "\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 200\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = [], []\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "\n",
    "        grad_x = f_grad(x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              -grad_x, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        x -= step * grad_x\n",
    "\n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, call the `test_solver` function with this solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_solver(gradient_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It runs the algorithm on three functions:\n",
    "- A non-convex Gaussian kernel ($f(x) = -\\exp(-\\|x\\|^2)$)\n",
    "- A badly conditioned quadratic function (but still strongly convex)\n",
    "- The Rosenbrock function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Implement Newton method\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b>\n",
    "     <ul>\n",
    "         <li>Implement Newton's method. Beware that the Hessian SHOULD be regularized !</li>\n",
    "         <li>Comment on what you observe. Do not describe the curves\n",
    "             but rather comment if convergence rates match theoretical results.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "\n",
    "def newton(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.00001\n",
    "    c2 = 0.95\n",
    "    max_iter = 100\n",
    "    lambda_threshold = 0.0001 Â # regularization threshold\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    for k in range(1, max_iter + 1):\n",
    "\n",
    "        grad_x = f_grad(x)\n",
    "        \n",
    "        # Compute the Hessian, regularize it and compute the search direction d\n",
    "        \n",
    "        # TODO H = ..\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute here the new value of x\n",
    "        x += step * d\n",
    "\n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solver(newton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR COMMENTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement DFP algorithm\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "         <li>Now, implement the DFP algorithm using the formula for $B$ in the slides.</li>\n",
    "         <li>Comment on what you observe. Focus on the explanation, not on describing the curves!</li>\n",
    "         <li>Isn't there a contradiction on the quadratic functions with what we've seen in class? What is going on?</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfp(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.95\n",
    "    max_iter = 200\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    B = np.eye(len(x))  # inverse Hessian approximation, start from Id\n",
    "    \n",
    "    grad_x = f_grad(x)\n",
    "    \n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        # Compute the search direction\n",
    "        d = np.dot(B, -grad_x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "        \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute the new value of x\n",
    "        s = step * d\n",
    "        x = x + s\n",
    "        y = new_grad - grad_x\n",
    "        ################################################################\n",
    "        # Update the inverse Hessian approximation\n",
    "        \n",
    "        # TODO B = ...\n",
    "        \n",
    "        ################################################################\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "        grad_x = new_grad\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solver(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR COMMENTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implement BFGS algorithm\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b>\n",
    "     <ul>\n",
    "         <li>You should now implement BFGS, using the formula for $B_t$ seen in the slides.</li>\n",
    "         <li>Comment on what you observe.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 100\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    B = np.eye(len(x))  # Hessian approximation\n",
    "    \n",
    "    grad_x = f_grad(x)\n",
    "    \n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        # Compute the search direction\n",
    "        d = -np.dot(B, grad_x)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "                \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute the new value of x\n",
    "        s = step * d\n",
    "        x += s\n",
    "        y = new_grad - grad_x\n",
    "        ##################################################################\n",
    "        # Update the inverse Hessian approximation\n",
    "        \n",
    "        # TODO B = \n",
    "        ##################################################################\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "        grad_x = new_grad\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solver(bfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR COMMENTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Implement L-BFGS algorithm\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 4:</b>\n",
    "     <ul>\n",
    "         <li>You should now implement the L-BFGS algorithm. We ask you to code the two-loops recursion in the function called two_loops.</li>\n",
    "         <li>Comment briefly on what you see.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** The question is hard. Consider this as a bonus question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_loops(grad_x, m, s_list, y_list, mu_list, B0):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    grad_x : ndarray, shape (n,)\n",
    "        gradient at the current point\n",
    "    \n",
    "    m : int\n",
    "        memory size\n",
    "    \n",
    "    s_list : list of length m\n",
    "        the past m values of s\n",
    "    \n",
    "    y_list : list of length m\n",
    "        the past m values of y\n",
    "\n",
    "    mu_list : list of length m\n",
    "        the past m values of mu\n",
    "        \n",
    "    B0 : ndarray, shape (n, n)\n",
    "        Initial inverse Hessian guess\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r :  ndarray, shape (n,)\n",
    "        the L-BFGS direction\n",
    "    '''\n",
    "    q = grad_x.copy()\n",
    "    alpha_list = []\n",
    "    # TODO : first loop\n",
    "    r = np.dot(B0, q)\n",
    "    # TODO: second loop\n",
    "    return -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbfgs(x0, f, f_grad, f_hessian):\n",
    "    default_step = 0.01\n",
    "    c1 = 0.0001\n",
    "    c2 = 0.9\n",
    "    max_iter = 100\n",
    "    m = 2\n",
    "    \n",
    "    # This variable is used to indicate whether or not we want to print\n",
    "    # monitoring information (iteration counter, function value and norm of the gradient)\n",
    "    verbose = False\n",
    "\n",
    "    all_x_k, all_f_k = list(), list()\n",
    "    x = x0\n",
    "\n",
    "    all_x_k.append(x.copy())\n",
    "    all_f_k.append(f(x))\n",
    "\n",
    "    B0 = np.eye(len(x))  # Hessian approximation\n",
    "    \n",
    "    grad_x = f_grad(x)\n",
    "    \n",
    "    y_list, s_list, mu_list = [], [], []\n",
    "    for k in range(1, max_iter + 1):       \n",
    "        \n",
    "        # Compute the search direction\n",
    "        d = two_loops(grad_x, m, s_list, y_list, mu_list, B0)\n",
    "\n",
    "        # Compute a step size using a line_search to satisfy the\n",
    "        # strong Wolfe conditions\n",
    "        step, _, _, new_f, _, new_grad = optimize.line_search(f, f_grad, x,\n",
    "                                                              d, grad_x,\n",
    "                                                              c1=c1, c2=c2)\n",
    "                \n",
    "        if step is None:\n",
    "            print(\"Line search did not converge at iteration %s\" % k)\n",
    "            step = default_step\n",
    "\n",
    "        # Compute the new value of x\n",
    "        s = step * d\n",
    "        x += s\n",
    "        y = new_grad - grad_x\n",
    "        mu = 1 / np.dot(y, s)\n",
    "        ##################################################################\n",
    "        # Update the memory\n",
    "        y_list.append(y.copy())\n",
    "        s_list.append(s.copy())\n",
    "        mu_list.append(mu)\n",
    "        if len(y_list) > m:\n",
    "            y_list.pop(0)\n",
    "            s_list.pop(0)\n",
    "            mu_list.pop(0)\n",
    "        ##################################################################\n",
    "        \n",
    "        all_x_k.append(x.copy())\n",
    "        all_f_k.append(new_f)\n",
    "\n",
    "        l_inf_norm_grad = np.max(np.abs(new_grad))\n",
    "\n",
    "        if verbose:\n",
    "            print('iter: %d, f: %.6g, l_inf_norm(grad): %.6g' %\n",
    "                  (k, new_f, l_inf_norm_grad))\n",
    "\n",
    "        if l_inf_norm_grad < 1e-6:\n",
    "            break\n",
    "            \n",
    "        grad_x = new_grad\n",
    "\n",
    "    return np.array(all_x_k), np.array(all_f_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_solver(lbfgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE YOUR COMMENTS HERE"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
