{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Lab 3 : Proximal/cyclic/greedy coordinate descent\n",
    "\n",
    "#### Authors: A. Gramfort, M. Massias, P. Ablin\n",
    "\n",
    "## Aim\n",
    "\n",
    "The aim of this material is to code \n",
    "- cyclic and greedy coordinate descent for ordinary least squares (OLS)\n",
    "- proximal coordinate descent for sparse Logistic regression\n",
    "\n",
    "## VERY IMPORTANT\n",
    "\n",
    "- This work **must be done by pairs of students**.\n",
    "- **Each** student must send their work **before the 23th of november at noon**, using the **moodle platform**.\n",
    "- This means that **each student in the pair sends the same file**\n",
    "- The **name of the file must be** constructed as in the next cell\n",
    "\n",
    "# Gentle reminder: no evaluation if you don't respect this EXACTLY\n",
    "\n",
    "### How to construct the name of your file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change here using YOUR first and last names\n",
    "fn1 = \"pierre\"\n",
    "ln1 = \"ablin\"\n",
    "fn2 = \"alexandre\"\n",
    "ln2 = \"gramfort\"\n",
    "\n",
    "filename = \"_\".join(map(lambda s: s.strip().lower(), \n",
    "                        [\"lab3\", ln1, fn1, \"and\", ln2, fn2])) + \".ipynb\"\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usual functions:\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def simu(coefs, n_samples=1000, corr=0.5, for_logreg=False):\n",
    "    n_features = len(coefs)\n",
    "    cov = toeplitz(corr ** np.arange(0, n_features))\n",
    "    A = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    b = A.dot(coefs) + randn(n_samples)\n",
    "    if for_logreg:\n",
    "        b = np.sign(b)\n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ordinary Least Squares\n",
    "\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^n$.\n",
    "We want to use coordinate descent to solve:\n",
    "    $$\\hat w \\in  \\mathrm{arg \\, min \\,} \\frac 12 \\Vert Aw - b \\Vert ^2 $$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b> We ask you to code\n",
    "     <ul>\n",
    "         <li>cyclic coordinate descent: at iteration $t$, update feature $j = t \\mod p$</li>\n",
    "         <li>greedy coordinate descent: at iteration $t$, update feature having the largest partial gradient in magnitude, ie $j = \\mathrm{arg\\, max \\,}_{i} \\vert \\nabla_i f(w_t) \\vert$.\n",
    "</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**WARNING**: You must do this in a clever way, ie such that $p$ updates cost the same as one update of GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "np.random.seed(1970)\n",
    "coefs = np.random.randn(n_features)\n",
    "\n",
    "A, b = simu(coefs, n_samples=1000, for_logreg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    residuals = b - A.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    lips_const = np.linalg.norm(A, axis=0) ** 2\n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        j = t % n_features\n",
    "        # TODO\n",
    "        # old_w_j = \n",
    "        # w[j] += \n",
    "        # update residuals:\n",
    "        # residuals \n",
    "        # END TODO\n",
    "        \n",
    "        if t % n_features == 0:\n",
    "            all_objs.append((residuals ** 2).sum() / 2.)\n",
    "    return w, np.array(all_objs)\n",
    "\n",
    "\n",
    "\n",
    "def greedy_cd(A, b, n_iter):\n",
    "    n_samples, n_features = A.shape\n",
    "    all_objs = []\n",
    "    \n",
    "    w = np.zeros(n_features)\n",
    "    \n",
    "    gradient = A.T.dot(A.dot(w) - b)\n",
    "    gram = A.T.dot(A)  # you will need this to keep the gradient up to date\n",
    "    \n",
    "    # TODO\n",
    "    # lips_const = \n",
    "    # END TODO \n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        # TODO\n",
    "        # choose feature j to update: \n",
    "        # j = \n",
    "        # old_w_j \n",
    "        # w[j] -= \n",
    "        # update gradient:\n",
    "        # gradient\n",
    "        # END TODO\n",
    "        s\n",
    "        if t % n_features == 0:\n",
    "            all_objs.append(0.5 * np.linalg.norm(A.dot(w) - b) ** 2)\n",
    "    \n",
    "    return w, np.array(all_objs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "         <li>Compute a precise minimum with your favorite solver</li>\n",
    "         <li>Compare the performance of cyclic and greedy CD as function of iterations.</li>\n",
    "         <li>From a practical point of view, could you use greedy CD for L2 regularized logistic regression? to solve OLS, but with 100,000 features? Explain your answers.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** You will do the plots using the number of iterations on the x-axis and not time as your code is likely to be slow unless you use [numba](https://numba.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sparse Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important result\n",
    "\n",
    "Remember: we are solving \n",
    "$$\\hat w \\in \\mathrm{arg \\, min} \\sum_{i=1}^{n} \\mathrm{log} ( 1 + e^{- y_i w^\\top x_i} )  + \\lambda \\Vert w \\Vert_1$$\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b><br/>\n",
    "    Assuming uniqueness of the solution, show that: $\\lambda \\geq \\lambda_{max} \\Leftrightarrow \\hat w = 0$\n",
    "where $\\lambda_{max} := \\frac 12 \\Vert X^\\top y \\Vert_{\\infty}$.\n",
    "</div>\n",
    "\n",
    "**HINT:** You will need the following beautiful result: for any $w =(w_1, \\dots, w_p) \\in \\mathbb{R}^p$, the subdifferential of the L1 norm at $w$ is:\n",
    "\n",
    "$$\\partial \\Vert \\cdot \\Vert_1 (w) = \\partial \\vert \\cdot \\vert (w_1)  \\times \\dots \\times \\partial \\vert \\cdot \\vert (w_p) $$\n",
    "where $\\times$ is the Cartesian product between sets,\n",
    "and $$ \\partial \\vert \\cdot \\vert (w_j) = \n",
    "\\begin{cases} &w_j / |w_j| &\\mathrm{if} \\quad w_j \\neq 0, \n",
    "         \\\\ & [-1, 1] &\\mathrm{otherwise.} \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "(it should now be easy to find $\\partial \\Vert \\cdot \\Vert_1 (\\mathbf{0}_p)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 4:</b><br/>\n",
    "    Show that for sparse Logistic regression the coordinate-wise Lipschitz constant of the smooth term, $\\gamma_j$, can be taken equal to $\\Vert X_j \\Vert^2 / 4$, where $X_j$ denotes the $j$-th column of $X$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 5:</b><br/>\n",
    "    Code cyclic proximal coordinate descent for sparse Logistic regression:\n",
    "</div>\n",
    "\n",
    "**WARNING**: the Lasso means linear regression (quadratic fitting term) with L1 penalty. Sparse logistic regression means logistic regression with L1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = simu(coefs, n_samples=1000, for_logreg=True)\n",
    "lambda_max = norm(X.T.dot(y), ord= np.inf) / 2.\n",
    "lamb = lambda_max / 20.  \n",
    "# much easier to parametrize lambda as a function of lambda_max than \n",
    "# to take random values like 0.1 in previous Labs\n",
    "\n",
    "\n",
    "def sigmoid(t):\n",
    "    \"\"\"Sigmoid function\"\"\"\n",
    "    return 1. / (1. + np.exp(-t))\n",
    "\n",
    "\n",
    "def soft_thresh(x, u):\n",
    "    \"\"\"Soft thresholding of x at level u\"\"\"\n",
    "    return np.sign(x) * np.maximum(0., np.abs(x) - u)\n",
    "\n",
    "\n",
    "def cd_logreg(X, y, lamb, n_iter):\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    Xw = X.dot(w)\n",
    "    \n",
    "    # TODO\n",
    "    # lips_const = \n",
    "    # END TODO\n",
    "    \n",
    "    for t in range(n_iter):\n",
    "        for j in range(n_features):\n",
    "            old_w_j = w[j]\n",
    "            # TODO\n",
    "            # grad_j = \n",
    "            # w[j] = soft_thresh(1, 2)\n",
    "            \n",
    "            # if old_w_j != w[j]:\n",
    "                # Xw += \n",
    "            #END TODO\n",
    "            \n",
    "        all_objs[t] = np.log(1. + np.exp(-y * Xw)).sum() + lamb * norm(w, ord=1)\n",
    "    \n",
    "    return w, all_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Real data\n",
    "\n",
    "We will compare vanilla cyclic CD and ISTA to solve the Lasso on a real dataset, called _leukemia_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "leuk = fetch_openml(\"leukemia\")\n",
    "\n",
    "X = np.asfortranarray(leuk.data)\n",
    "y = np.ones(leuk.target.shape)\n",
    "y[leuk.target == leuk.target[0]] = -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "\n",
    "lambda_max_lasso = norm(X.T.dot(y), ord=np.inf)\n",
    "lambd = lambda_max_lasso / 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 6:</b> Code\n",
    "    <ul>\n",
    "        <li>a simple proximal gradient solver for the Lasso</li>\n",
    "        <li>a prox CD solver for the Lasso and compare them on this dataset.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** Do the plots in terms of epochs, not updates (to be fair to CD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
