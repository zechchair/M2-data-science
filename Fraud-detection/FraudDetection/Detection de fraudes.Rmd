---
title: "Detection de fraudes"
author: "zakaria echchair"
date: "23/04/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(eval = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(cache = TRUE)
```

## prepare dataset

```{r import libraries}
library(reticulate) # permet d'utiliser Rmarkdown
library(caTools) #call library for split data
set.seed(123)#random seed to mix data
library(ModelMetrics)
options(digits=20)
library(dplyr)  
library(ggplot2)
library(mltools)
library(data.table)
# Load CART packages
library(rpart)
library(rpart.plot)
library(caret)
#install.packages("e1071")
library(e1071)
library(randomForest)
library(arules)
library(gbm)
options(reticulate.repl.quiet = TRUE)
```

Apres avoir importer les bibliotheques, nous commençons par l’importation d’un sous ensemble de la base de données ou il n’y a que les transfers et les cash_out (afin de reduire le nombre total des observations). on souligne que dans le contexte de “data balancing”, le sous ensemble importé contient tout les fraudes qui existent dans la base de données originale. et il a une taille de 2.77 millions d’observations au lieu de 6.qlq.


```{r Importation du sous ensemble de la data}
df = read.csv('G:/My Drive/statistics/data/Fraud.csv')
df=df[ which(df$type == 'CASH_OUT' | df$type == 'TRANSFER'  ), ]
dim(df)
```

Notre data set contient 2770409 (2.77 millions) observations et 11
variables

## Traitement de la base des données

Nous effectuons chi-squared test pour eliminer les variables qui n'ont
aucune relation avec isFrarud( notre dependent variable)

```{r Testes statistique pour eliminer les variables inutiles,warning = FALSE}
sprintf("chi-squared p value pour nameOrig est: %s supérieur au seuil 0.05 donc on va admettre H0 (il n'y a aucune relation entre nameOrig et isFraud) ", round(chisq.test(df$nameOrig, df$isFraud, correct=TRUE)$p.value, digits=2))
sprintf("chi-squared p value pour nameDest est: %s inférieur au seuil 0.05 donc on va rejeter H0 (il n'y a aucune relation entre nameDest et isFraud) alors il y a une relations entre eux  ", round(chisq.test(df$nameDest, df$isFraud, correct=TRUE)$p.value, digits=2))
sprintf("chi-squared p value pour type est: %s inférieur au seuil 0.05 donc on va rejeter H0 (il n'y a aucune relation entre type et isFraud) alors il y a une relations entre eux ", round(chisq.test(df$type, df$isFraud, correct=TRUE)$p.value, digits=2))

```

Certes p-value de "nameDest" est inferieur au seuil, cependant on
elimine cette variable parce que le nom du destinataire est unique,
autrement dit les gens qui effectuent les frauds vont pas utiliser
toujours les comptes banquaires, et en meme temps il ne faut pas lier
les fraudes à des comptes specifiques. Alors on garde la variable "type"
parce que elle est significante, par contre on supprime les deux
varriables "nameDest" et "nameOrig". De plus on elimine la variable
"isFlaggedFraud" parce que elle signifie que la machine a detecter un
fraud, cependant nous voulons detecter les frauds par notre algorithme
au lieu d'exploiter un autre algorithme dejà implementé.

```{r Suppression des variables inutiles}
df=subset(df, select=-c(isFlaggedFraud,nameOrig,nameDest))

```

On definie la variable "type" en tant qu"une variable categorique

```{r Definition des categories}
df$type=as.factor(df$type)
```

Nous presentons une description de notre base de données final

```{r Description de la data}
summary(df)

```

## Équilibrage de la base des données

La technique de "undersampling "

Nous proposons une discrétisation des variables qui sont trés
importantes en plusieurs categories(qu'on choisie soigneusement), pour
effectuer une division de la base de données en gardant la meme
structure de la base initiale .

A cet egard on on va creer deux nouvelles variables catégoriques
"amount_cat" et "step_cat"

```{r Discrétisation}

df$amount_cat=discretize(df$amount, method = "fixed", breaks = c(0,1000,10000,100000,1000000,10000000,max(df$amount)))
df$step_cat=discretize(df$step, method = "fixed", breaks = c(1,48,96,144,192,240,288,336,384,432,480,528,576,624,672,720))

```

Puis on concatener ces variables dans une nouvelle "new" qui regroupe
les informations liées à "amount_cat" et "step_cat"

On propose de diviser notre data à deux, une base de données ou il n'y a
que les fraudes et une autre ou il y a les non fraudes

```{r Serparation des fraudes et des non fraudes}
df$new=paste(df$amount_cat,df$step_cat)
df_isFraud=df[ which(df$isFraud == 1), ]
df_isNotFraud=df[ which(df$isFraud == 0 ), ]
```

Nous prenons un sous ensemble de la base des non fraudes de 10000
observations qui represente la majorité de la base de données originale.

Ps. la fonction "createDataPartition" prend une partition de la dataset
on gardant les memes pourcentages des valeures dans variable "new".
cette fonction n'admet qu'une seule variable "new" comme argument.

Alors le raisonement sur "df_isNotFraud\$new" equivalent au raisonnement
sur "amount_cat" et "step_cat" en meme temps.

```{r Creation du sous ensemble des non fraudes,warning = FALSE}
my.ids <- createDataPartition(df_isNotFraud$new, p = 10000/nrow(df_isNotFraud))
df_small <- df_isNotFraud[as.numeric(my.ids[[1]]), ]
nrow(df_small)
```

Nous remarquons que la partition se constitue de 10048 au lieu de 10000.
parce que cette fonctions cherche toujours un compromis entre le
pourcentages des valeurs de "new et le nombre des observations dans le
sous ensemble.

Les graphiques suivants representent le rapport entre la proportion de
chaque categorie dans la base de donnée initiale (avant le split) et
finale (aprés le split)

```{r}

plot(prop.table(table(df_isNotFraud$step_cat))/prop.table(table(df_small$step_cat)) ,ylab="Le rapport de la proportion de [step] dans l'extrait sur celle dans la data initial( not fraud)",xlab="Categories de step",main="Conservation de la structure de la data")

plot(prop.table(table(df_isNotFraud$amount_cat))/prop.table(table(df_small$amount_cat)),ylab="Le rapport de la proportion de [amount] dans l'extrait sur celle dans la data initial( not fraud)",xlab="Categories de amount",main="Conservation de la structure de la data" )



```

Nous remarquons que le rapport est preque 1 (100%) sauf pour quelque
categories parce que les observations de ses categories sont trés peu.
et c'est pas evident d'extraire 10000 observations parmis 2.77 millions
en gardant le meme pourcentage des categories.




```{r }
split=sample.split(df_isFraud$new,SplitRatio = 0.75) 
df_isFraud_train = subset(df_isFraud, split==TRUE)
df_isFraud_test = subset(df_isFraud, split==FALSE)

split=sample.split(df_isNotFraud$isFraud,SplitRatio = 0.75) 
df_isNotFraud_train = subset(df_isNotFraud, split==TRUE)
df_isNotFraud_test = subset(df_isNotFraud, split==FALSE)

split=sample.split(df_small$isFraud,SplitRatio = 0.75) 
df_small_train = subset(df_small, split==TRUE)
df_small_test = subset(df_small, split==FALSE)
```


Ensuite on concatène l’extrait 75% des 10000 observations des non frauds avec 75% des frauds. en tant que **small training set**.
et les 25% qui restes des non frauds avec ce qui reste des frauds en tant que **small testing set**
et de meme on concatène 75% des non frauds globals avec 75% des frauds en tant que **big training set** et bien evidement le reste en tant que **big testing set**.

```{r}
Train_big=rbind(df_isNotFraud_train,df_isFraud_train)
Train_big=Train_big[order(Train_big$step),]

Test_big=rbind(df_isNotFraud_test,df_isFraud_test)
Test_big=Test_big[order(Test_big$step),]

Train_small=rbind(df_small_train,df_isFraud_train)
Train_small=Train_small[order(Train_small$step),]

Test_small=rbind(df_small_test,df_isFraud_test)
Test_small=Test_small[order(Test_small$step),]
```


Ps.nous optons pour cette technique afin qu’on puisse generer des testings sets où les observations “frauds” sont toutes nouvelles pour tous les trainings set en même temps.
Nous obtenons traning and testing sets “Train_small” et “Test_small”
Et de meme pour la base de données globale nous obtenons “Train_big” et “Test_big”


Finalement, on supprime les variables categoriques qui nous etaient d’une extreme importance pour les split.

```{r Suppression des variables categoriques}
Train_small = subset(Train_small ,select=-c(new,amount_cat,step_cat))
df_isFraud = subset(df_isFraud ,select=-c(new,amount_cat,step_cat))
df_isNotFraud = subset(df_isNotFraud ,select=-c(new,amount_cat,step_cat))
Test_small=subset(Test_small ,select=-c(new,amount_cat,step_cat))
Train_big = subset(Train_big ,select=-c(new,amount_cat,step_cat))
Test_big = subset(Test_big ,select=-c(new,amount_cat,step_cat))
df = subset(df ,select=-c(new,amount_cat,step_cat))
df_isFraud_train = subset(df_isFraud_train ,select=-c(new,amount_cat,step_cat))
df_isFraud_test = subset(df_isFraud_test ,select=-c(new,amount_cat,step_cat))
df_isNotFraud_test = subset(df_isNotFraud_test ,select=-c(new,amount_cat,step_cat))
df_isNotFraud_train = subset(df_isNotFraud_train ,select=-c(new,amount_cat,step_cat))
df_small_train = subset(df_small_train ,select=-c(new,amount_cat,step_cat))
df_small_test = subset(df_small_test ,select=-c(new,amount_cat,step_cat))
df_small = subset(df_small ,select=-c(amount_cat,step_cat,new))
```


## Logistic model

nous construisons un modèle logistique par le biais du “Train_small” (le training set du df_small ou plutot la data extraite de la base de données initiale) , ce modéle est nomé “model_log_small”
et nous construisons un autre model basé sur le grand testing set “Train_big” nomé “model_log_big”

```{r Creation des modeles logistiques}
model_log_small=glm(isFraud ~ . , data=Train_small,family = binomial)
summary(model_log_small)
model_log_big=glm(isFraud ~ . , data=Train_big,family = binomial)
summary(model_log_big)
```

Nous remarquons que toutes les variables sont significatives (trois etoiles dans le model *** avec une tres petite p-value)

On peut rien deduire du AIC vu que les modeles sont entrainnés sur des differentes datasets


Pour determiner le meilleur threshold, nous traçons la courbe ROC (true positives rate and false positives rate in fuction of threshold)


```{r}
library(ROCR)
predict_roc_big= predict(model_log_big,type="response")
ROCRpred_big = prediction(predict_roc_big,Train_big$isFraud)
ROCRperf_big =performance(ROCRpred_big,"tpr","fpr")
plot(ROCRperf_big,colorize=FALSE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7),main='Big data ROC curves')
predict_roc_small= predict(model_log_small,type="response")
ROCRpred_small = prediction(predict_roc_small,Train_small$isFraud)
ROCRperf_small =performance(ROCRpred_small,"tpr","fpr")
plot(ROCRperf_small,colorize=TRUE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7),main='Small data ROC curves')

```

Notre objectif est de maximiser “true positive rate” et minimiser “false positive rate” 
c’est pour cela nous optons pour 0.7 en tant que threshold
nous expliquons la densité des thresholds dans le premier graph “big data ROC curves”, par le fait que le nombre des non fraudes (2.77 million) est tres grand par rapport au frauds ( 8213 ) (unbalancing data issues) . Alors meme si le nombre des faux positifs est grand, son pourcentage reste toujours tres faible, autrement dit "false positives rate" reste toujours tres petit quelque soit le threshold.


On calcule la surface sous la courbe (AREA UNDER CURVE AUR) qui donne une idée à propos de la qualité absolue du model(100% for perfect model)


```{r}
as.numeric(performance(ROCRpred_big, "auc")@y.values)

as.numeric(performance(ROCRpred_small, "auc")@y.values)

```
le premier model explique 98% des observations, et le deuxieme explique juste 97%.

```{r}

threshold=0.7
print("big data predictions on big testing set")
predictions= predict(model_log_big,type="response",newdata = Test_big)
a=table(Test_big$isFraud,predictions>threshold)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))


print("-----------------------------------------------------")
print("big data predictions on small testing set")
predictions= predict(model_log_big,type="response",newdata = Test_small)
a=table(Test_small$isFraud,predictions>threshold)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))

print("-----------------------------------------------------")
print("small data predictions on big testing set")
predictions= predict(model_log_small,type="response",newdata = Test_big)
a=table(Test_big$isFraud,predictions>threshold)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))

print("-----------------------------------------------------")
print("small data predictions on small testing set")
predictions= predict(model_log_small,type="response",newdata = Test_small)
a=table(Test_small$isFraud,predictions>threshold)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))

```

**overall accuracy** n’est pas suffisante en tant que critere d’evaluation, vu qu’elle est trop liée à la taille du testing set. ils croissent simultanément.
et de meme **overall error** decroit inversement à la taille du testing set.
mais ces deux criteres sont tres importants quant aux testings sets qui ont la meme taille.
*the rate of detected fraud* est aussi un critere tres important mais ne pas suffisant. puisqu’il donne une idée à propos du pourcentage des fraudes detectés mais il ne note pas le taux des non fraudes qui sont detectés comme fraudes.
c’est pour cela il faut toujours comparer tout ces criteres pour choisir le meilleur modele.
A cet egard, nous remarquons que le *small model* donne toujours une accuracy meilleure que celle du *big model*. En effet, quand on entrainne notre model sur la grande data, il perd de la credibilité surtout parce que cette data est inequilibrée, par contre la small base de données est plus credible et de plus équilibrée.

## Deuxieme technique d’equilbre de data

Nous divisons la data des non fraudes à plusieurs datasets de 10000 observations concatenées avec 75% des frauds . qui vont etre tous des trainings sets
Ps.les memes frauds se trouvent dans tous ces sous-ensembles.
Et finalement on prend une base de données ou il y a le reste des non frauds (un nombre inférieur à 10000) et 25% des fraudes qui ne se trouvent pas dans les trainings sets.



```{r }
notFraud_c=df_isNotFraud
i=1
dt=list()
while (dim(notFraud_c)[1]>10000){
  split=sample.split(notFraud_c$step,SplitRatio = 10000/nrow(notFraud_c)) 
part = subset(notFraud_c, split==TRUE)
notFraud_c = subset(notFraud_c, split==FALSE)
dt[[i]]=rbind(part,df_isFraud_train)
dt[[i]]=dt[[i]][order(dt[[i]]$step),]
  i=i+1
}

test_rest=rbind(notFraud_c,df_isFraud_test)
test_rest=test_rest[order(test_rest$step),]

```
pour eviter l’execution de ces split à chaque fois, on exporte ces datasets dans un dossier exterieur.
et on les importe à chaque fois.
```{r Exporter ou importer la data,include=FALSE,eval=FALSE}
#dt=list()

for (i in 0:274){
  str=sprintf('G:/My Drive/statistics/data/sans_drop/isFraud%d.csv',i)
  dt[[i+1]]=read.csv(str)

  #str=sprintf('G:/My Drive/statistics/data/from_R/isFraud%d.csv',i)
  #write.csv(data[[i+1]],str, row.names = FALSE)
}

#write.csv(test_rest,'G:/My Drive/statistics/data/from_R/test_rest.csv', row.names = FALSE)
##write.csv(df_isFraud_test,'G:/My Drive/statistics/data/from_R/isFraud%d.csv', row.names = FALSE)
#df_isFraud_test=read.csv('G:/My Drive/statistics/data/from_R/df_isFraud_test.csv')

```
nous construions 274 model logistique, chaqu’un s’entrainne sur une nouvelle data set equilibrée ou il y 10000 observation non frauds et 6092 frauds (75% des frauds)
la prediction finale sera la moyenne des predictions des autres modeles sur le meme testing set (qui contient tout les not fraud et juste 25% des frauds)

```{r 270 model}
model_log=list()
for (i in 1:270){
model_log[[i]]=glm(isFraud ~ ., data=dt[[i]],family = binomial)

}

```
```{r}
sum_pred=0
for (i in 1:270){
sum_pred=sum_pred+predict(model_log[[i]],type="response",newdata = Test_big)}
sum_pred=sum_pred/270

```

```{r}
ROCRpred_big = prediction(sum_pred,Test_big$isFraud)
ROCRperf_big =performance(ROCRpred_big,"tpr","fpr")
plot(ROCRperf_big,colorize=FALSE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7),main='Big data ROC curves')
```

```{r}
a=table(Test_big$isFraud,(sum_pred)>0.6)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))
sprintf("the rate of not fraud predicted as fraud is: %s  " , round((((a[3])/(a[1]+a[3]))*100), digits=2))
```

Nous remarquons que les resultats de cette technique sont plus faible que le modele logiqtique avec la small data(qui garde la meme structure de la data global)
dans cette technique chaque modele donne une probabilté que une observation sera fraud, et nous calculons la moyenne de ces probabilités pour la meme observation. le resultat sera notre proba finale que l’observation soit une fraude.
nous choisissons 0.6 en tant que threshold.


## Decision tree


De meme nous construisons deux models de decision trees, le premier sur small training set et le deuxieme sur le big traning set.

```{r}
model_tree_small = rpart(isFraud ~ ., data = Train_small, method="class",cp=0.001)
prp(model_tree_small)
model_tree_big = rpart(isFraud ~ . , data = Train_big, method="class",cp=0.1)
prp(model_tree_big)
```
les arbres au dessus donnent une interpretation logique et developée du probleme.

```{r}
library(ROCR)
predict_roc_big= predict(model_tree_big,type="prob")
ROCRpred_big = prediction(predict_roc_big[,2],Train_big$isFraud)
ROCRperf_big =performance(ROCRpred_big,"tpr","fpr")
plot(ROCRperf_big,colorize=TRUE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7),main="ROC curves big data")

predict_roc_small= predict(model_tree_small,type="prob")
ROCRpred_small = prediction(predict_roc_small[,2],Train_small$isFraud)
ROCRperf_small =performance(ROCRpred_small,"tpr","fpr")
plot(ROCRperf_small,colorize=TRUE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7),main="ROC curves small data")



```

selon ROC curves et pour maximiser les Tpr et minimiser les fpr, nous choisissons 0.5 en tant que threshold.

```{r}
as.numeric(performance(ROCRpred_big, "auc")@y.values)

as.numeric(performance(ROCRpred_small, "auc")@y.values)

```

le premier modele explique 82% des observations et le deuxieme modele (small data) explique 99.5% des observations. Autrement dit le model sur la small data est plus efficace que celui sur la big data.

```{r}

threshold_tree=0.5
print("big data predictions on big testing set")
predictions= predict(model_tree_big,type="prob",newdata = Test_big)
a=table(Test_big$isFraud,predictions[,2]>threshold_tree)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))

print("------------------------------------------")
print("big data predictions on small testing set")
predictions= predict(model_tree_big,type="prob",newdata = Test_small)
a=table(Test_small$isFraud,predictions[,2]>threshold_tree)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))
print("------------------------------------------")
print("small data predictions on big testing set")
predictions= predict(model_tree_small,type="prob",newdata = Test_big)
a=table(Test_big$isFraud,predictions[,2]>threshold_tree)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))
print("------------------------------------------")
print("small data predictions on small testing set")
predictions= predict(model_tree_small,type="prob",newdata = Test_small)
a=table(Test_small$isFraud,predictions[,2]>threshold_tree)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))
print("------------------------------------------")
```


nous remarquons que le “small model” donne toujours un resultat meilleur que celle du “big model”. c’est la meme chose que l’interpretation de la partie logistique.
En guise de conclusion, decision tree models donnent une accuracy rate of detected fraud tres importante par rapport à celle de logistique modele,mais malheuresement on sacrifie overall accuracy, qui se diminue legereme, cette dimunition coute beaucoup de false positives surtout quand le nombre des non frauds est tres grand par rapport au fraud ( umblanced data)


et par la technique de la moyenne des predictions (ou bien voting)


```{r}
model_tree=list()
sum_pred=0
for (i in 1:270){
model_tree[[i]]=rpart(isFraud ~ ., data = dt[[i]], method="class",cp=0.001)
}
for (i in 1:270){
sum_pred=sum_pred+predict(model_tree[[i]],type="prob",newdata = Test_big)[,2]}

a=table(Test_big$isFraud,(sum_pred/270)>0.5)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))

```
```{r}
predict_roc_big=0
for (i in 1:270){
predict_roc_big=predict_roc_big+predict(model_tree[[i]],type="prob")[,2]}
ROCRpred_big = prediction(predict_roc_big[,2],Train_big$isFraud)
ROCRperf_big =performance(ROCRpred_big,"tpr","fpr")
plot(ROCRperf_big,colorize=TRUE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7),main="ROC curves big data")

```

cette fois-ci la methode du voting donne des resultats plus importatnts que les precedents avec rate of detected fraud de 99.42% et overall accuracy de 98.64%. 



## gbm

```{r}

model_gauss_small=gbm(isFraud~. ,data=Train_small, distribution="bernoulli",n.trees =5000,interaction.depth=4,shrinkage=0.05)
summary(model_gauss_small)
model_gauss_big=gbm(isFraud~. ,data=Train_big, distribution="bernoulli",n.trees =1000,interaction.depth=2)
summary(model_gauss_big)
```
d'apres le rapport nous remarquons que la variable oldbalanceOrg est la plus significante dans les deux modeles "small model" et "big model"


```{r}
library(ROCR)
predict_roc_big= predict(model_gauss_big,type="response")
ROCRpred_big = prediction(predict_roc_big,Train_big$isFraud)
ROCRperf_big =performance(ROCRpred_big,"tpr","fpr")
plot(ROCRperf_big,colorize=FALSE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7))

predict_roc_small= predict(model_gauss_small,type="response")
ROCRpred_small = prediction(predict_roc_small,Train_small$isFraud)
ROCRperf_small =performance(ROCRpred_small,"tpr","fpr")
plot(ROCRperf_small,colorize=TRUE,print.cutoffs.at=seq(0,1,0.1),text.adj=c(-0.2,1.7))



```
d'apres ROC curves nous prenons le threshold = , afin de maximiser les tpr et minimiser les fpr (un petit taux de fpr coute beaucoup d'observation detectées comme fraudes meme s'elles ne sont pas, c'est pour cela nous minimisant le fpr meme par un petit taux)
et bien evidement le modele "small_gauss_model" eest presque parfait puisque son **ROC curves** a un angle de 90°
```{r}
as.numeric(performance(ROCRpred_big, "auc")@y.values)

as.numeric(performance(ROCRpred_small, "auc")@y.values)

```

le model"big_gauss model" explique plus de 68% de la data (parce que le nombre des arbres est inferieur mais avec ce nombre il prend plus de temps pour l'execution) par contre le model "small_gauss_model" explique 100% de la data

```{r}

threshold_gbm=0.9
print("big data predictions on big testing set")
predictions= predict(model_gauss_big,n.trees =1000,type='response',newdata = Test_big)
a=table(Test_big$isFraud,predictions>threshold_gbm)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))

print("----------------------------------------")
print("big data predictions on small testing set")
predictions= predict(model_gauss_big,n.trees =1000,type='response',newdata = Test_small)
a=table(Test_small$isFraud,predictions>threshold_gbm)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))
print("----------------------------------------")
print("small data predictions on big testing set")
predictions= predict(model_gauss_small,n.trees =5000,type='response',newdata = Test_big)
a=table(Test_big$isFraud,predictions>threshold_gbm)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))
print("----------------------------------------")
print("small data predictions on small testing set")
predictions= predict(model_gauss_small,n.trees =5000,type='response',newdata = Test_small)
a=table(Test_small$isFraud,predictions>threshold_gbm)
a
sprintf("the overall accuracy is: %s  " , round((((a[4]+a[1])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the overall error is: %s  " , round((((a[3]+a[2])/(a[2]+a[4]+a[1]+a[3]))*100), digits=2))
sprintf("the rate of detected fraud is: %s  " , round((((a[4])/(a[2]+a[4]))*100), digits=2))
print("----------------------------------------")
```


finalement nous trouvons que **le gradien boosting method** est le plus efficace, puisqu'il peut detecter 100% des fraudes avec une grande *overall accuracy* qui egale à %  ssauf avec le **small training set** qui s'execute dans le minimum du temps par rapport au autres, et bien evidement sans **voting methode**.

ces resultats sont grace à la particularité du **gbm* qui construit plusieurs arbres dependantes ( au contraire de __random_forest__ qui construit des arbres independantes entre elles). Et dans notre cas c'est un avantage parce que:

*GBM construit les arbres un par un, où chaque nouvel arbre aide à corriger les erreurs commises par l'arbre préalablement formé.*

Cela permet de trouver les anomalies avec la plus grande précision sans donner trop d'exemples authentiques aux experts.  l'augmentation du gradient s'est avérée être une méthode puissante sur des ensembles de données réels pour résoudre les problèmes d'apprentissage du classement en raison de ses deux caractéristiques principales:
Il effectue l'optimisation dans l'espace des fonctions (plutôt que dans l'espace des paramètres), ce qui facilite grandement l'utilisation des fonctions de perte personnalisées.
Le boosting se concentre pas à pas sur des exemples difficiles qui donnent une belle stratégie pour gérer les ensembles de données déséquilibrés en renforçant l'impact de la classe positive.

le boosting a montrer des performances tres elevées par rapport aux autres methodes pour la detection des anomalies surtout dans des data non equilibrées, ce qui veut dire que ce modele est le meilleur pour la detection des fraudes meme s'il est relativement plus long que les autres en termes du temps d'execution. Cependant grace a notre strategie d'equilibrement de la data, nous pouvions reduire enormement ce temps d'execution avec les meilleurs resultats.

