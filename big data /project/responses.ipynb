{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_black\n",
      "23/01/08 22:45:54 WARN Utils: Your hostname, zechchair-gl553vd resolves to a loopback address: 127.0.1.1; using 192.168.0.130 instead (on interface wlp2s0)\n",
      "23/01/08 22:45:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/08 22:45:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name\n",
    "%load_ext jupyter_black\n",
    "spark = SparkSession.builder.appName(\"project1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "my_path = \"/home/zechchair/Documents/github/M2-data-science/big data /project/input\"\n",
    "only_files = [my_path + \"/\" + f for f in listdir(my_path) if isfile(join(my_path, f))]\n",
    "files_names = [f for f in listdir(my_path) if isfile(join(my_path, f))]\n",
    "onlyCities_names = set(\n",
    "    [files_name.split(\".\")[0].split(\"_\")[0] for files_name in files_names]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, reverse, when, lit, avg, sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+------+-----+\n",
      "|   _c0|      city|         store_N|income|month|\n",
      "+------+----------+----------------+------+-----+\n",
      "|JAN 13|      lyon|        lyon.txt|    13|  JAN|\n",
      "|FEB 12|      lyon|        lyon.txt|    12|  FEB|\n",
      "|MAR 14|      lyon|        lyon.txt|    14|  MAR|\n",
      "|APR 15|      lyon|        lyon.txt|    15|  APR|\n",
      "|MAY 12|      lyon|        lyon.txt|    12|  MAY|\n",
      "|JUN 15|      lyon|        lyon.txt|    15|  JUN|\n",
      "|JUL 19|      lyon|        lyon.txt|    19|  JUL|\n",
      "|AUG 25|      lyon|        lyon.txt|    25|  AUG|\n",
      "|SEP 13|      lyon|        lyon.txt|    13|  SEP|\n",
      "|OCT 11|      lyon|        lyon.txt|    11|  OCT|\n",
      "|NOV 22|      lyon|        lyon.txt|    22|  NOV|\n",
      "|DEC 22|      lyon|        lyon.txt|    22|  DEC|\n",
      "|JAN 21|marseilles|marseilles_1.txt|    21|  JAN|\n",
      "|FEB 21|marseilles|marseilles_1.txt|    21|  FEB|\n",
      "|MAR 21|marseilles|marseilles_1.txt|    21|  MAR|\n",
      "|APR 27|marseilles|marseilles_1.txt|    27|  APR|\n",
      "|MAY 25|marseilles|marseilles_1.txt|    25|  MAY|\n",
      "|JUN 25|marseilles|marseilles_1.txt|    25|  JUN|\n",
      "|JUL 21|marseilles|marseilles_1.txt|    21|  JUL|\n",
      "|AUG 22|marseilles|marseilles_1.txt|    22|  AUG|\n",
      "+------+----------+----------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").load(my_path + \"/*.txt\")\n",
    "\n",
    "# Add a column with the file name\n",
    "df = df.withColumn(\"city\", input_file_name())\n",
    "df = df.withColumn(\"store_N\", reverse(split(\"city\", \"/\"))[0])\n",
    "df = df.withColumn(\"city\", split(\"store_N\", \"\\\\.\")[0])\n",
    "df = df.withColumn(\"city\", split(\"city\", \"_\")[0])\n",
    "df = df.withColumn(\"income\", df[\"_c0\"].substr(5, 7))\n",
    "df = df.withColumn(\"month\", df[\"_c0\"].substr(0, 3))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      avg(income)|\n",
      "+-----------------+\n",
      "|23.19871794871795|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg(\"income\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|       avg(income)|\n",
      "+-----+------------------+\n",
      "|  APR| 20.23076923076923|\n",
      "|  OCT| 26.53846153846154|\n",
      "|  NOV| 24.53846153846154|\n",
      "|  FEB|19.153846153846153|\n",
      "|  SEP| 25.53846153846154|\n",
      "|  JAN| 20.76923076923077|\n",
      "|  AUG|23.076923076923077|\n",
      "|  MAR| 17.53846153846154|\n",
      "|  DEC|              29.0|\n",
      "|  JUN|27.846153846153847|\n",
      "|  JUL|21.692307692307693|\n",
      "|  MAY| 22.46153846153846|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"month\").agg(avg(\"income\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| city|month|avg(income)|\n",
      "+-----+-----+-----------+\n",
      "|anger|  AUG|       15.0|\n",
      "|anger|  MAY|       12.0|\n",
      "|anger|  SEP|       13.0|\n",
      "|anger|  APR|       15.0|\n",
      "|anger|  MAR|       14.0|\n",
      "|anger|  NOV|       14.0|\n",
      "|anger|  JAN|       13.0|\n",
      "|anger|  JUN|       15.0|\n",
      "|anger|  JUL|       19.0|\n",
      "|anger|  DEC|       16.0|\n",
      "|anger|  OCT|        8.0|\n",
      "|anger|  FEB|       12.0|\n",
      "| lyon|  MAR|       14.0|\n",
      "| lyon|  AUG|       25.0|\n",
      "| lyon|  APR|       15.0|\n",
      "| lyon|  MAY|       12.0|\n",
      "| lyon|  OCT|       11.0|\n",
      "| lyon|  JUL|       19.0|\n",
      "| lyon|  FEB|       12.0|\n",
      "| lyon|  JUN|       15.0|\n",
      "+-----+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"city\", \"month\").agg(avg(\"income\")).orderBy(\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      city|       avg(income)|\n",
      "+----------+------------------+\n",
      "|     anger|13.833333333333334|\n",
      "|      lyon|16.083333333333332|\n",
      "|marseilles|21.458333333333332|\n",
      "|    nantes|             17.25|\n",
      "|      nice|16.916666666666668|\n",
      "|    orlean|16.333333333333332|\n",
      "|     paris| 43.55555555555556|\n",
      "|    rennes|              15.0|\n",
      "|  toulouse|             14.75|\n",
      "|    troyes|17.833333333333332|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"city\").agg(avg(\"income\")).orderBy(\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      city|       avg(income)|\n",
      "+----------+------------------+\n",
      "|     anger|13.833333333333334|\n",
      "|      lyon|16.083333333333332|\n",
      "|marseilles|21.458333333333332|\n",
      "|    nantes|             17.25|\n",
      "|      nice|16.916666666666668|\n",
      "|    orlean|16.333333333333332|\n",
      "|     paris| 43.55555555555556|\n",
      "|    rennes|              15.0|\n",
      "|  toulouse|             14.75|\n",
      "|    troyes|17.833333333333332|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"city\").agg(avg(\"income\")).orderBy(\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-----------+\n",
      "|month|         store_N|sum(income)|\n",
      "+-----+----------------+-----------+\n",
      "|  DEC|        lyon.txt|       22.0|\n",
      "|  SEP|marseilles_1.txt|       23.0|\n",
      "|  OCT|        lyon.txt|       11.0|\n",
      "|  MAY|        lyon.txt|       12.0|\n",
      "|  JAN|marseilles_1.txt|       21.0|\n",
      "|  FEB|marseilles_1.txt|       21.0|\n",
      "|  APR|        lyon.txt|       15.0|\n",
      "|  MAR|marseilles_1.txt|       21.0|\n",
      "|  APR|marseilles_1.txt|       27.0|\n",
      "|  AUG|marseilles_1.txt|       22.0|\n",
      "|  OCT|marseilles_1.txt|       28.0|\n",
      "|  NOV|        lyon.txt|       22.0|\n",
      "|  JUL|        lyon.txt|       19.0|\n",
      "|  JUN|        lyon.txt|       15.0|\n",
      "|  JUL|marseilles_1.txt|       21.0|\n",
      "|  JUN|marseilles_1.txt|       25.0|\n",
      "|  MAR|        lyon.txt|       14.0|\n",
      "|  MAY|marseilles_1.txt|       25.0|\n",
      "|  NOV|marseilles_1.txt|       24.0|\n",
      "|  DEC|marseilles_1.txt|       26.0|\n",
      "+-----+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"month\", \"store_N\").agg(sum(\"income\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-----------+\n",
      "|month|         store_N|avg(income)|\n",
      "+-----+----------------+-----------+\n",
      "|  DEC|        lyon.txt|       22.0|\n",
      "|  SEP|marseilles_1.txt|       23.0|\n",
      "|  OCT|        lyon.txt|       11.0|\n",
      "|  MAY|        lyon.txt|       12.0|\n",
      "|  JAN|marseilles_1.txt|       21.0|\n",
      "|  FEB|marseilles_1.txt|       21.0|\n",
      "|  APR|        lyon.txt|       15.0|\n",
      "|  MAR|marseilles_1.txt|       21.0|\n",
      "|  APR|marseilles_1.txt|       27.0|\n",
      "|  AUG|marseilles_1.txt|       22.0|\n",
      "|  OCT|marseilles_1.txt|       28.0|\n",
      "|  NOV|        lyon.txt|       22.0|\n",
      "|  JUL|        lyon.txt|       19.0|\n",
      "|  JUN|        lyon.txt|       15.0|\n",
      "|  JUL|marseilles_1.txt|       21.0|\n",
      "|  JUN|marseilles_1.txt|       25.0|\n",
      "|  MAR|        lyon.txt|       14.0|\n",
      "|  MAY|marseilles_1.txt|       25.0|\n",
      "|  NOV|marseilles_1.txt|       24.0|\n",
      "|  DEC|marseilles_1.txt|       26.0|\n",
      "+-----+----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"month\", \"store_N\").agg(avg(\"income\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+\n",
      "|month|    store_N|max_income|\n",
      "+-----+-----------+----------+\n",
      "|  APR| rennes.txt|         9|\n",
      "|  APR|   nice.txt|         9|\n",
      "|  AUG|paris_2.txt|        45|\n",
      "|  DEC|paris_1.txt|        71|\n",
      "|  FEB|paris_2.txt|        42|\n",
      "|  JAN|paris_1.txt|        51|\n",
      "|  JUL|paris_1.txt|        61|\n",
      "|  JUN|paris_2.txt|        85|\n",
      "|  MAR|paris_2.txt|        44|\n",
      "|  MAY|paris_2.txt|        72|\n",
      "|  NOV|paris_2.txt|        64|\n",
      "|  OCT| orlean.txt|         8|\n",
      "|  OCT|  anger.txt|         8|\n",
      "|  SEP|paris_2.txt|        63|\n",
      "+-----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, reverse, when, lit, avg, sum, max, col\n",
    "\n",
    "# Group the data by the month column\n",
    "grouped_df = df.groupBy(\"month\")\n",
    "\n",
    "# Find the maximum income for each group\n",
    "max_df = grouped_df.agg(max(col(\"income\")).alias(\"max_income\"))\n",
    "max_df = max_df.withColumnRenamed(\"month\", \"Month_max\")\n",
    "\n",
    "\n",
    "# Join the original data with the aggregated data on the month and income columns\n",
    "joined_df = df.join(\n",
    "    max_df,\n",
    "    (df[\"month\"] == max_df[\"Month_max\"]) & (df[\"income\"] == max_df[\"max_income\"]),\n",
    ")\n",
    "\n",
    "# Select the month, store, and max_income columns\n",
    "result_df = joined_df.select(\"month\", \"store_N\", \"max_income\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TCP connection...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m s\u001b[39m.\u001b[39mlisten(\u001b[39m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mWaiting for TCP connection...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m conn, addr \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39;49maccept()\n\u001b[1;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mConnected... Starting getting tweets.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m resp \u001b[39m=\u001b[39m get_tweets()\n",
      "File \u001b[0;32m~/anaconda3/envs/optimisation/lib/python3.9/socket.py:293\u001b[0m, in \u001b[0;36msocket.accept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccept\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    287\u001b[0m     \u001b[39m\"\"\"accept() -> (socket object, address info)\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \n\u001b[1;32m    289\u001b[0m \u001b[39m    Wait for an incoming connection.  Return a new socket\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    representing the connection, and the address of the client.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[39m    For IP sockets, the address info is a pair (hostaddr, port).\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     fd, addr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accept()\n\u001b[1;32m    294\u001b[0m     sock \u001b[39m=\u001b[39m socket(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfamily, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproto, fileno\u001b[39m=\u001b[39mfd)\n\u001b[1;32m    295\u001b[0m     \u001b[39m# Issue #7995: if no default timeout is set and the listening\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[39m# socket had a (non-zero) timeout, force the new socket in blocking\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[39m# mode to override platform-specific socket flags inheritance.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import sys\n",
    "import requests\n",
    "import requests_oauthlib\n",
    "import json\n",
    "\n",
    "# def create_twitter_stream(ssc):\n",
    "# Replace these with your own access tokens\n",
    "CONSUMER_KEY = \"J32dJ5o0oyt4XobzJ32yE7igX\"\n",
    "CONSUMER_SECRET = \"pH9P9d1sqtJv2BBCrh8MXnx19vaSWaMiqp9yYD42ZTayMZkzZB\"\n",
    "ACCESS_TOKEN = \"1208952534662078464-CDWpV6ZFH0B2gvMnpXHWcsZaXRn4Rh\"\n",
    "ACCESS_TOKEN_SECRET = \"JvEKx3ZcAEnaKzjF2R8cCFYVsmHm9xkBBbYFiSYc90lsc\"\n",
    "my_auth = requests_oauthlib.OAuth1(\n",
    "    CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    ")\n",
    "\n",
    "\n",
    "def get_tweets():\n",
    "    url = \"https://stream.twitter.com/1.1/statuses/filter.json\"\n",
    "    query_data = [(\"language\", \"en\"), (\"locations\", \"-130,-20,100,50\"), (\"track\", \"#\")]\n",
    "    query_url = url + \"?\" + \"&\".join([str(t[0]) + \"=\" + str(t[1]) for t in query_data])\n",
    "    response = requests.get(query_url, auth=my_auth, stream=True)\n",
    "    print(query_url, response)\n",
    "    return response\n",
    "\n",
    "\n",
    "def send_tweets_to_spark(http_resp, tcp_connection):\n",
    "    for line in http_resp.iter_lines():\n",
    "        try:\n",
    "            full_tweet = json.loads(line)\n",
    "            tweet_text = full_tweet[\"text\"]\n",
    "            print(\"Tweet Text: \" + tweet_text)\n",
    "            print(\"------------------------------------------\")\n",
    "            tcp_connection.send(tweet_text + \"\\n\")\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(\"Error: %s\" % e)\n",
    "\n",
    "\n",
    "TCP_IP = \"localhost\"\n",
    "TCP_PORT = 9009\n",
    "conn = None\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "s.bind((TCP_IP, TCP_PORT))\n",
    "s.listen(1)\n",
    "print(\"Waiting for TCP connection...\")\n",
    "conn, addr = s.accept()\n",
    "print(\"Connected... Starting getting tweets.\")\n",
    "resp = get_tweets()\n",
    "send_tweets_to_spark(resp, conn)\n",
    "\n",
    "# we create this class that inherits from the StreamListener in tweepy StreamListener\n",
    "# class TweetListener(tweepy.Stream): # NEW CODE\n",
    "\n",
    "#     def __init__(self, csocket):\n",
    "#         self.client_socket = csocket\n",
    "#     # we override the on_data() function in StreamListener\n",
    "#     def on_data(self, data):\n",
    "#         try:\n",
    "#             message = json.loads( data )\n",
    "#             print( message['text'].encode('utf-8') )\n",
    "#             self.client_socket.send( message['text'].encode('utf-8') )\n",
    "#             return True\n",
    "#         except BaseException as e:\n",
    "#             print(\"Error on_data: %s\" % str(e))\n",
    "#         return True\n",
    "\n",
    "#     def if_error(self, status):\n",
    "#         print(status)\n",
    "#         return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/09 01:56:34 WARN Utils: Your hostname, zechchair-gl553vd resolves to a loopback address: 127.0.1.1; using 192.168.0.130 instead (on interface wlp2s0)\n",
      "23/01/09 01:56:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/09 01:56:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zechchair/anaconda3/envs/optimisation/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row,SQLContext\n",
    "import sys\n",
    "import requests\n",
    "# create spark configuration\n",
    "conf = SparkConf()\n",
    "conf.setAppName(\"TwitterStreamApp\")\n",
    "# create spark context with the above configuration\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# create the Streaming Context from the above spark context with interval size 2 seconds\n",
    "ssc = StreamingContext(sc, 2)\n",
    "# setting a checkpoint to allow RDD recovery\n",
    "ssc.checkpoint(\"checkpoint_TwitterApp\")\n",
    "# read data from port 9009\n",
    "dataStream = ssc.socketTextStream(\"localhost\",9009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each tweet into words\n",
    "words = dataStream.flatMap(lambda line: line.split(\" \"))\n",
    "# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)\n",
    "hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))\n",
    "# adding the count of each hashtag to its last count\n",
    "tags_totals = hashtags.updateStateByKey(aggregate_tags_count)\n",
    "# do processing for each RDD generated in each interval\n",
    "tags_totals.foreachRDD(process_rdd)\n",
    "# start the streaming computation\n",
    "ssc.start()\n",
    "# wait for the streaming to finish\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DStream' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lines\u001b[39m.\u001b[39;49mshow()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DStream' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "def aggregate_tags_count(new_values, total_sum):\n",
    "\treturn sum(new_values) + (total_sum or 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_context_instance(spark_context):\n",
    "    if ('sqlContextSingletonInstance' not in globals()):\n",
    "        globals()['sqlContextSingletonInstance'] = SQLContext(spark_context)\n",
    "        return globals()['sqlContextSingletonInstance']\n",
    "def process_rdd(time, rdd):\n",
    "    print(\"----------- %s -----------\" % str(time))\n",
    "    try:\n",
    "        sql_context = get_sql_context_instance(rdd.context)\n",
    "        # convert the RDD to Row RDD\n",
    "        row_rdd = rdd.map(lambda w: Row(hashtag=w[0], hashtag_count=w[1]))\n",
    "        # create a DF from the Row RDD\n",
    "        hashtags_df = sql_context.createDataFrame(row_rdd)\n",
    "        # Register the dataframe as table\n",
    "        hashtags_df.registerTempTable(\"hashtags\")\n",
    "        # get the top 10 hashtags from the table using SQL and print them\n",
    "        hashtag_counts_df = sql_context.sql(\"select hashtag, hashtag_count from hashtags order by hashtag_count desc limit 10\")\n",
    "        hashtag_counts_df.show()\n",
    "        # call this method to prepare top 10 hashtags DF and send them\n",
    "        send_df_to_dashboard(hashtag_counts_df)\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        print(\"Error: %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_df_to_dashboard(df):\n",
    "\t# extract the hashtags from dataframe and convert them into array\n",
    "\ttop_tags = [str(t.hashtag) for t in df.select(\"hashtag\").collect()]\n",
    "\t# extract the counts from dataframe and convert them into array\n",
    "\ttags_count = [p.hashtag_count for p in df.select(\"hashtag_count\").collect()]\n",
    "\t# initialize and send the data through REST API\n",
    "\turl = 'http://localhost:5001/updateData'\n",
    "\trequest_data = {'label': str(top_tags), 'data': str(tags_count)}\n",
    "\tresponse = requests.post(url, data=request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "class TweetListener(tweepy.Stream): # NEW CODE\n",
    "\n",
    "    def __init__(self, csocket):\n",
    "        self.client_socket = csocket\n",
    "    # we override the on_data() function in StreamListener\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            message = json.loads( data )\n",
    "            print( message['text'].encode('utf-8') )\n",
    "            self.client_socket.send( message['text'].encode('utf-8') )\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    "\n",
    "    def if_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "def create_twitter_stream(ssc):\n",
    "  # Replace these with your own access tokens\n",
    "    CONSUMER_KEY    = \"DHwxvBBBebmi1l4NID1Zj3Zgh\"\n",
    "    CONSUMER_SECRET = \"LZwdVVHn2UWsQQsZAFil6m6v8UHA2PEnYJEKmmZi8tBNK82Akl\"\n",
    "    ACCESS_TOKEN       = \"1208952534662078464-liYjvgXoLMHFjdi90DaJAPYSaxP7dB\"\n",
    "    ACCESS_TOKEN_SECRET= \"WIKf8GvUreMpJmbYov1fDnFX8x8wUqpU6FGO2S6fKcTJT\"\n",
    "\n",
    "    # Set up the Twitter API client\n",
    "    # auth = tweepy.OAuth1UserHandler(CONSUMER_KEY, CONSUMER_SECRET,\n",
    "    #                                 ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "    # api = tweepy.API(auth)\n",
    "\n",
    "    # # Create a DStream of tweets using the Twitter API\n",
    "    # tweets_dstream = tweepy.Stream(auth=api.auth, listener=tweepy.TweepyUtils.TweetReceiver())\n",
    "    auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "\n",
    "    twitter_stream = Stream(auth, TweetsListener(c_socket))\n",
    "    twitter_stream.filter(track=['football']) #we are interested in this topic.\n",
    "  return tweets_dstream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/09 02:01:46 WARN Utils: Your hostname, zechchair-gl553vd resolves to a loopback address: 127.0.1.1; using 192.168.0.130 instead (on interface wlp2s0)\n",
      "23/01/09 02:01:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/09 02:01:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tweepy' has no attribute 'TweepyUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m ssc \u001b[39m=\u001b[39m StreamingContext(sc, \u001b[39m10\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Create a DStream of tweets using the create_twitter_stream function\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tweets_dstream \u001b[39m=\u001b[39m create_twitter_stream(ssc)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Extract the hashtags from each tweet\u001b[39;00m\n\u001b[1;32m     14\u001b[0m hashtags_dstream \u001b[39m=\u001b[39m tweets_dstream\u001b[39m.\u001b[39mflatMap(\u001b[39mlambda\u001b[39;00m tweet: tweet\u001b[39m.\u001b[39mhashtags)\n",
      "Cell \u001b[0;32mIn [1], line 16\u001b[0m, in \u001b[0;36mcreate_twitter_stream\u001b[0;34m(ssc)\u001b[0m\n\u001b[1;32m     13\u001b[0m api \u001b[39m=\u001b[39m tweepy\u001b[39m.\u001b[39mAPI(auth)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Create a DStream of tweets using the Twitter API\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m tweets_dstream \u001b[39m=\u001b[39m tweepy\u001b[39m.\u001b[39;49mTweepyUtils\u001b[39m.\u001b[39mcreateStream(ssc, api)\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m tweets_dstream\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tweepy' has no attribute 'TweepyUtils'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a SparkContext and a StreamingContext\n",
    "sc = SparkContext(appName=\"PopularHashtags\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tweepy' has no attribute 'TweepyUtils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m ssc \u001b[39m=\u001b[39m StreamingContext(sc, \u001b[39m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Create a DStream of tweets using the create_twitter_stream function\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tweets_dstream \u001b[39m=\u001b[39m create_twitter_stream(ssc)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Extract the hashtags from each tweet\u001b[39;00m\n\u001b[1;32m      7\u001b[0m hashtags_dstream \u001b[39m=\u001b[39m tweets_dstream\u001b[39m.\u001b[39mflatMap(\u001b[39mlambda\u001b[39;00m tweet: tweet\u001b[39m.\u001b[39mhashtags)\n",
      "Cell \u001b[0;32mIn [7], line 35\u001b[0m, in \u001b[0;36mcreate_twitter_stream\u001b[0;34m(ssc)\u001b[0m\n\u001b[1;32m     32\u001b[0m api \u001b[39m=\u001b[39m tweepy\u001b[39m.\u001b[39mAPI(auth)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Create a DStream of tweets using the Twitter API\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m tweets_dstream \u001b[39m=\u001b[39m tweepy\u001b[39m.\u001b[39mStream(auth\u001b[39m=\u001b[39mapi\u001b[39m.\u001b[39mauth, listener\u001b[39m=\u001b[39mtweepy\u001b[39m.\u001b[39;49mTweepyUtils\u001b[39m.\u001b[39mTweetReceiver())\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m tweets_dstream\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tweepy' has no attribute 'TweepyUtils'"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# Create a DStream of tweets using the create_twitter_stream function\n",
    "tweets_dstream = create_twitter_stream(ssc)\n",
    "\n",
    "# Extract the hashtags from each tweet\n",
    "hashtags_dstream = tweets_dstream.flatMap(lambda tweet: tweet.hashtags)\n",
    "\n",
    "# Count the number of occurrences of each hashtag over a sliding window of 10 minutes\n",
    "hashtag_counts_dstream = hashtags_dstream.map(lambda hashtag: (hashtag, 1)).reduceByKeyAndWindow(lambda x, y: x + y, lambda x, y: x - y, 600, 10)\n",
    "\n",
    "# Sort the RDD by the count in descending order and take the top 10 hashtags\n",
    "top_hashtags_dstream = hashtag_counts_dstream.transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False).take(10))\n",
    "\n",
    "# Print the top hashtags to the console\n",
    "top_hashtags_dstream.foreachRDD(lambda rdd: print(\"\\nTop hashtags:\", rdd.collect()))\n",
    "\n",
    "# Start the streaming context and wait for termination\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Set up a timer to update the plot every 10 seconds\n",
    "def update_plot(hashtags_counts_rdd):\n",
    "  # Get the top 10 hashtags and their counts\n",
    "  top_hashtags = hashtags_counts_rdd.take(10)\n",
    "  hashtags = [x[0] for x in top_hashtags]\n",
    "  counts = [x[1] for x in top_hashtags]\n",
    "\n",
    "  # Update the plot with the new data\n",
    "  ax.clear()\n",
    "  ax.bar(hashtags, counts)\n",
    "  plt.xticks(rotation=90)\n",
    "  fig.canvas.draw()\n",
    "\n",
    "hashtags_counts.foreachRDD(update_plot)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Start the streaming process\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimisation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d27a5b15d50a0b87e597ec8b04607d26a776244cd01a86df1daaa5551e6e6a91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
